{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1SzyWjzjhqC",
        "outputId": "c4fcb015-4f8d-4f18-8050-b9653b821805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pandas numpy matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install \"nvidia-modelopt[all]\" -U --extra-index-url https://pypi.nvidia.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5yHq_0kUnEkA"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zruaxu6jlhUQ",
        "outputId": "b487b01d-1bb1-432c-df9e-31f5d2a3623e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Kcl-Jj_x1m",
        "outputId": "6518c1e3-8d9a-4281-d156-fe1eaff9fabd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-10.9.0.34.tar.gz (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12==10.9.0.34 (from tensorrt)\n",
            "  Downloading tensorrt_cu12-10.9.0.34.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12_libs==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt)\n",
            "  Downloading tensorrt_cu12_libs-10.9.0.34.tar.gz (704 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12_bindings==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt)\n",
            "  Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt_cu12_libs==10.9.0.34->tensorrt_cu12==10.9.0.34->tensorrt) (12.4.127)\n",
            "Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt_cu12_libs\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.9.0.34-py2.py3-none-any.whl size=46629 sha256=457c171dc70a9bcda1c58ac5f6157fd898fd7b7cfee6fa31d1f21c468d206b75\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/4d/72/f28cb367f1435d026243047d4f60fde8f1c9cbb06a204f842f\n",
            "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.9.0.34-py2.py3-none-any.whl size=17465 sha256=d19175e2dcf4d3d4dea0b002ef04dc0a4d2afe012411bf07bb5d537e48be3e1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/09/76/6b405075fe4c04097f5713ec0a688df7892aaee823bc141952\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12_libs: filename=tensorrt_cu12_libs-10.9.0.34-py2.py3-none-manylinux_2_28_x86_64.whl size=3103291777 sha256=4a82f0bda2874596f202f6edc8dae99b86a3c4ec2fa142a9c847c4d3a57864a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/d0/06/35d7b3006eead25828debb658da848328ebfd38962a2bcd096\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt_cu12_libs\n",
            "Installing collected packages: tensorrt_cu12_bindings, tensorrt_cu12_libs, tensorrt_cu12, tensorrt\n",
            "Successfully installed tensorrt-10.9.0.34 tensorrt_cu12-10.9.0.34 tensorrt_cu12_bindings-10.9.0.34 tensorrt_cu12_libs-10.9.0.34\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --upgrade tensorrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z6HP5mVj6Kc",
        "outputId": "0dfffdea-1a15-42a2-85d4-00b780977dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading isic-2019.zip to /home/ubuntu\n",
            "100%|██████████████████████████████████████▉| 9.09G/9.10G [01:02<00:00, 176MB/s]\n",
            "100%|███████████████████████████████████████| 9.10G/9.10G [01:02<00:00, 156MB/s]\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "!kaggle datasets download andrewmvd/isic-2019\n",
        "!unzip -q isic-2019.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfBncMXtiC75",
        "outputId": "f944e445-5eab-46d2-c1b6-e93252baf6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.6)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnxsim) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from mako->pycuda) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytools-2025.1.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1-cp311-cp311-linux_x86_64.whl size=660393 sha256=b4da248574d098676932bdd9513ee92a3344e937601d3a7863e5846c5e3e5e1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/7e/6c/d2d1451ea6424cdc3d67b36c16fa7111eafdf2034bc3405666\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, onnx, mako, pycuda, onnxsim\n",
            "Successfully installed mako-1.3.9 onnx-1.17.0 onnxsim-0.4.36 pycuda-2025.1 pytools-2025.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda onnx onnxsim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uvIBd2bsegY4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/quant/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchinfo import summary\n",
        "import modelopt.torch.quantization as mtq\n",
        "import copy\n",
        "import modelopt.torch.opt as mto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "vcgQRMYhiHPn",
        "outputId": "618057be-b7fc-4b66-c296-b6f293468008"
      },
      "outputs": [],
      "source": [
        "# TensorRT and CUDA\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit  # Initializes CUDA context\n",
        "\n",
        "import onnx\n",
        "from onnxsim import simplify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OfIr73lyeh3A"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Organize Data ---\n",
        "# Load metadata\n",
        "train_meta = pd.read_csv('ISIC_2019_Training_GroundTruth.csv')\n",
        "train_meta['image'] = train_meta['image'] + '.jpg'\n",
        "\n",
        "# Convert one-hot encoding to class labels\n",
        "classes = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n",
        "train_meta['label'] = train_meta[classes].idxmax(axis=1)\n",
        "label_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "LR_HEAD = 1e-3\n",
        "LR_BACKBONE = 3e-5\n",
        "EPOCHS = 20\n",
        "CKPT_DIR = \"checkpoints\"\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT = 0.2\n",
        "BLOCKS = 6\n",
        "MODEL_NAME = \"dinov2_vits14\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4oYEH2aBj7xD"
      },
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(train_meta, test_size=0.3,\n",
        "                                    stratify=train_meta['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5,\n",
        "                                  stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# --- Step 3: Define Dataset Class ---\n",
        "class ISIC2019Dataset(Dataset):\n",
        "    def __init__(self, df, img_dir='ISIC_2019_Training_Input/ISIC_2019_Training_Input', transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_to_idx = label_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.label_to_idx[self.df.iloc[idx]['label']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fshz4BxQj95f",
        "outputId": "aaedbacb-3906-4c13-9900-a1bfe62a6e6b"
      },
      "outputs": [],
      "source": [
        "# --- Step 4: Data Transforms and Loaders ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create datasets and loaders\n",
        "batch_size = BATCH_SIZE\n",
        "train_dataset = ISIC2019Dataset(train_df, transform=train_transform)\n",
        "val_dataset = ISIC2019Dataset(val_df, transform=val_transform)\n",
        "test_dataset = ISIC2019Dataset(test_df, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zIG4o9GAfVYF"
      },
      "outputs": [],
      "source": [
        "# --- Step 6: Model Setup ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Step 5: Handle Class Imbalance ---\n",
        "train_labels = [label for _, label in train_dataset]\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYjSSyvtj_G6",
        "outputId": "9d037416-477c-4398-feae-883351f863a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        }
      ],
      "source": [
        "class DINOv2Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=8, model_name='dinov2_vitl14', hidden_dim=512, dropout=0.2, blocks=4):\n",
        "        super().__init__()\n",
        "        self.dinov2 = torch.hub.load('facebookresearch/dinov2', model_name)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.dinov2.embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "        for param in self.dinov2.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        for blk in self.dinov2.blocks[-blocks:]:\n",
        "            for p in blk.parameters():\n",
        "                p.requires_grad = True\n",
        "            # if hasattr(blk, 'norm1'):\n",
        "            #     blk.norm1.requires_grad = True\n",
        "        if hasattr(self.dinov2, \"norm\"):\n",
        "            for p in self.dinov2.norm.parameters():\n",
        "                p.requires_grad = True\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get all image features (batch_size, num_patches, embed_dim)\n",
        "        features = self.dinov2(x)\n",
        "\n",
        "        # Handle different output formats:\n",
        "        if features.dim() == 3:  # Standard case with spatial dimensions\n",
        "            cls_token = features[:, 0, :]  # Extract [CLS] token\n",
        "        else:  # Fallback for 2D output\n",
        "            cls_token = features\n",
        "\n",
        "        return self.classifier(cls_token)\n",
        "\n",
        "model = DINOv2Classifier(num_classes=len(classes), model_name=MODEL_NAME, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, blocks=BLOCKS).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "===================================================================================================================================================================================================\n",
              "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Trainable                 Kernel Shape              Mult-Adds\n",
              "===================================================================================================================================================================================================\n",
              "DINOv2Classifier                              [128, 3, 224, 224]        [128, 8]                  --                        Partial                   --                        --\n",
              "├─DinoVisionTransformer: 1-1                  [128, 3, 224, 224]        [128, 384]                526,848                   Partial                   --                        --\n",
              "│    └─PatchEmbed: 2-1                        [128, 3, 224, 224]        [128, 256, 384]           --                        False                     --                        --\n",
              "│    │    └─Conv2d: 3-1                       [128, 3, 224, 224]        [128, 384, 16, 16]        (226,176)                 False                     [14, 14]                  7,411,335,168\n",
              "│    │    └─Identity: 3-2                     [128, 256, 384]           [128, 256, 384]           --                        --                        --                        --\n",
              "│    └─ModuleList: 2-2                        --                        --                        --                        Partial                   --                        --\n",
              "│    │    └─NestedTensorBlock: 3-3            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-1               [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-2         [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-1             [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-2            [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-3             [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-4            [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-3              [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-4               [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-5                     [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-5             [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-6               [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-7            [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-8             [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-9            [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-6              [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-4            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-7               [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-8         [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-10            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-11           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-12            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-13           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-9              [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-10              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-11                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-14            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-15              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-16           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-17            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-18           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-12             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-5            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-13              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-14        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-19            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-20           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-21            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-22           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-15             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-16              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-17                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-23            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-24              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-25           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-26            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-27           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-18             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-6            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-19              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-20        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-28            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-29           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-30            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-31           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-21             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-22              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-23                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-32            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-33              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-34           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-35            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-36           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-24             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-7            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-25              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-26        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-37            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-38           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-39            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-40           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-27             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-28              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-29                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-41            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-42              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-43           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-44            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-45           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-30             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-8            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-31              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-32        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-46            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-47           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-48            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-49           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-33             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-34              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-35                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-50            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-51              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-52           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-53            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-54           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-36             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-9            [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-37              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-38        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-55            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-56           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-57            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-58           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-39             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-40              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-41                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-59            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-60              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-61           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-62            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-63           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-42             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-10           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-43              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-44        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-64            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-65           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-66            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-67           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-45             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-46              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-47                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-68            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-69              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-70           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-71            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-72           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-48             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-11           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-49              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-50        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-73            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-74           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-75            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-76           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-51             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-52              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-53                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-77            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-78              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-79           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-80            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-81           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-54             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-12           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-55              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-56        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-82            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-83           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-84            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-85           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-57             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-58              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-59                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-86            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-87              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-88           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-89            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-90           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-60             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-13           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-61              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-62        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-91            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-92           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-93            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-94           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-63             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-64              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-65                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-95            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-96              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-97           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-98            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-99           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-66             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-14           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-67              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-68        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-100           [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-101          [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-102           [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-103          [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-69             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-70              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-71                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-104           [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-105             [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-106          [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-107           [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-108          [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-72             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    └─LayerNorm: 2-3                         [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    └─Identity: 2-4                          [128, 384]                [128, 384]                --                        --                        --                        --\n",
              "├─Sequential: 1-2                             [128, 384]                [128, 8]                  --                        True                      --                        --\n",
              "│    └─Linear: 2-5                            [128, 384]                [128, 512]                197,120                   True                      --                        25,231,360\n",
              "│    └─GELU: 2-6                              [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─Dropout: 2-7                           [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─Linear: 2-8                            [128, 512]                [128, 8]                  4,104                     True                      --                        525,312\n",
              "===================================================================================================================================================================================================\n",
              "Total params: 22,257,800\n",
              "Trainable params: 10,853,384\n",
              "Non-trainable params: 11,404,416\n",
              "Total mult-adds (Units.GIGABYTES): 10.16\n",
              "===================================================================================================================================================================================================\n",
              "Input size (MB): 77.07\n",
              "Forward/backward pass size (MB): 15967.07\n",
              "Params size (MB): 86.92\n",
              "Estimated Total Size (MB): 16131.06\n",
              "==================================================================================================================================================================================================="
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape = (batch_size, 3, 224, 224)\n",
        "summary(model, (input_shape), device=device, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\", \"kernel_size\", \"mult_adds\"], depth=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8efHFigmkCZL",
        "outputId": "bb2e3ea8-6d71-4ec3-cd4c-07f417bef887"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=EPOCHS, quantized=False, name=\"best_dinov2.pth\"):\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=True)\n",
        "\n",
        "        for images, labels in train_loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            train_loop.set_postfix({'loss': f\"{loss.item():.4f}\", 'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"})\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_loop = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', leave=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loop:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_loop.set_postfix({\n",
        "                    'loss': f\"{loss.item():.4f}\",\n",
        "                    'acc': f\"{correct/total:.4f}\"\n",
        "                })\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = correct / total\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            directory = os.path.join(CKPT_DIR, name)\n",
        "            if quantized:\n",
        "                mto.save(model, directory)\n",
        "                pass\n",
        "            else:\n",
        "                torch.save(model.state_dict(), directory)\n",
        "            print(f\"\\nNew best model saved with val acc: {val_acc:.4f}\")\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.02it/s, loss=1.9706, lr=0.001000]\n",
            "Epoch 1/20 [Val]: 100%|██████████| 30/30 [00:15<00:00,  1.97it/s, loss=1.0035, acc=0.6126]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.6126\n",
            "\n",
            "Epoch 1 Summary:\n",
            "Train Loss: 1.4298 | Val Loss: 1.0601 | Val Acc: 0.6126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.02it/s, loss=0.9810, lr=0.000994]\n",
            "Epoch 2/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.41it/s, loss=0.6593, acc=0.7100]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7100\n",
            "\n",
            "Epoch 2 Summary:\n",
            "Train Loss: 1.0850 | Val Loss: 0.8909 | Val Acc: 0.7100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 139/139 [01:06<00:00,  2.08it/s, loss=1.0528, lr=0.000976]\n",
            "Epoch 3/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.35it/s, loss=0.7792, acc=0.7366]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7366\n",
            "\n",
            "Epoch 3 Summary:\n",
            "Train Loss: 0.9316 | Val Loss: 0.9243 | Val Acc: 0.7366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.02it/s, loss=0.4687, lr=0.000946]\n",
            "Epoch 4/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.35it/s, loss=0.5046, acc=0.7387]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7387\n",
            "\n",
            "Epoch 4 Summary:\n",
            "Train Loss: 0.8645 | Val Loss: 0.9832 | Val Acc: 0.7387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 139/139 [01:11<00:00,  1.94it/s, loss=0.3648, lr=0.000905]\n",
            "Epoch 5/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.36it/s, loss=0.6284, acc=0.7503]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7503\n",
            "\n",
            "Epoch 5 Summary:\n",
            "Train Loss: 0.7560 | Val Loss: 0.8190 | Val Acc: 0.7503\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 139/139 [01:07<00:00,  2.06it/s, loss=0.7023, lr=0.000854]\n",
            "Epoch 6/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.39it/s, loss=0.6322, acc=0.6708]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Summary:\n",
            "Train Loss: 0.7275 | Val Loss: 0.8108 | Val Acc: 0.6708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 139/139 [01:07<00:00,  2.05it/s, loss=0.7454, lr=0.000794]\n",
            "Epoch 7/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.42it/s, loss=0.6018, acc=0.7266]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 Summary:\n",
            "Train Loss: 0.6583 | Val Loss: 0.7668 | Val Acc: 0.7266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 139/139 [01:07<00:00,  2.06it/s, loss=0.5398, lr=0.000727]\n",
            "Epoch 8/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.43it/s, loss=0.6160, acc=0.7189]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 Summary:\n",
            "Train Loss: 0.6361 | Val Loss: 0.7596 | Val Acc: 0.7189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.13it/s, loss=0.4295, lr=0.000655]\n",
            "Epoch 9/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.35it/s, loss=0.4588, acc=0.7768]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7768\n",
            "\n",
            "Epoch 9 Summary:\n",
            "Train Loss: 0.5666 | Val Loss: 0.7604 | Val Acc: 0.7768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.11it/s, loss=0.3208, lr=0.000578]\n",
            "Epoch 10/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.41it/s, loss=0.5643, acc=0.7966]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7966\n",
            "\n",
            "Epoch 10 Summary:\n",
            "Train Loss: 0.4966 | Val Loss: 0.7100 | Val Acc: 0.7966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 139/139 [01:06<00:00,  2.09it/s, loss=0.3439, lr=0.000500]\n",
            "Epoch 11/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.36it/s, loss=0.4427, acc=0.7853]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11 Summary:\n",
            "Train Loss: 0.4560 | Val Loss: 0.7640 | Val Acc: 0.7853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 139/139 [01:04<00:00,  2.14it/s, loss=0.3989, lr=0.000422]\n",
            "Epoch 12/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.39it/s, loss=0.5244, acc=0.7992]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7992\n",
            "\n",
            "Epoch 12 Summary:\n",
            "Train Loss: 0.4296 | Val Loss: 0.6618 | Val Acc: 0.7992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 139/139 [01:07<00:00,  2.05it/s, loss=0.5609, lr=0.000345]\n",
            "Epoch 13/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.38it/s, loss=0.5853, acc=0.8113]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8113\n",
            "\n",
            "Epoch 13 Summary:\n",
            "Train Loss: 0.3705 | Val Loss: 0.7260 | Val Acc: 0.8113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.13it/s, loss=0.4025, lr=0.000273]\n",
            "Epoch 14/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.37it/s, loss=0.4589, acc=0.8266]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8266\n",
            "\n",
            "Epoch 14 Summary:\n",
            "Train Loss: 0.3435 | Val Loss: 0.7348 | Val Acc: 0.8266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.02it/s, loss=0.2218, lr=0.000206]\n",
            "Epoch 15/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.36it/s, loss=0.4820, acc=0.8176]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15 Summary:\n",
            "Train Loss: 0.3243 | Val Loss: 0.7181 | Val Acc: 0.8176\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.03it/s, loss=0.2807, lr=0.000146]\n",
            "Epoch 16/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.39it/s, loss=0.4637, acc=0.8032]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16 Summary:\n",
            "Train Loss: 0.3012 | Val Loss: 0.6620 | Val Acc: 0.8032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 139/139 [01:10<00:00,  1.97it/s, loss=0.1910, lr=0.000095]\n",
            "Epoch 17/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.43it/s, loss=0.4900, acc=0.8253]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17 Summary:\n",
            "Train Loss: 0.2863 | Val Loss: 0.6976 | Val Acc: 0.8253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.03it/s, loss=0.2952, lr=0.000054]\n",
            "Epoch 18/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.36it/s, loss=0.4810, acc=0.8353]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8353\n",
            "\n",
            "Epoch 18 Summary:\n",
            "Train Loss: 0.2654 | Val Loss: 0.7615 | Val Acc: 0.8353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 139/139 [01:04<00:00,  2.14it/s, loss=0.2812, lr=0.000024]\n",
            "Epoch 19/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.36it/s, loss=0.4254, acc=0.8345]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19 Summary:\n",
            "Train Loss: 0.2658 | Val Loss: 0.7798 | Val Acc: 0.8345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 139/139 [01:09<00:00,  2.00it/s, loss=0.1990, lr=0.000006]\n",
            "Epoch 20/20 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.32it/s, loss=0.4365, acc=0.8379]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8379\n",
            "\n",
            "Epoch 20 Summary:\n",
            "Train Loss: 0.2439 | Val Loss: 0.7756 | Val Acc: 0.8379\n"
          ]
        }
      ],
      "source": [
        "params_ft = [\n",
        "    {\"params\": model.classifier.parameters(),                             \"lr\": LR_HEAD},\n",
        "    {\"params\": [p for p in model.dinov2.parameters() if p.requires_grad], \"lr\": LR_BACKBONE},\n",
        "]\n",
        "optimizer = optim.AdamW(params_ft)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a0i-1GuoD_D",
        "outputId": "fe9dd3ab-c59b-487a-e5ff-e8de71108a63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/home/ubuntu/checkpoints/best_dinov2.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:72: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert H % patch_H == 0, f\"Input image height {H} is not a multiple of patch height {patch_H}\"\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:73: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert W % patch_W == 0, f\"Input image width {W} is not a multiple of patch width: {patch_W}\"\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:183: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if npatch == N and w == h:\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:191: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  M = int(math.sqrt(N))  # Recover the number of patches in each dimension\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:192: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert N == M * M\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:197: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sx = float(w0 + self.interpolate_offset) / M\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:198: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sy = float(h0 + self.interpolate_offset) / M\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:209: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (w0, h0) == patch_pos_embed.shape[-2:]\n"
          ]
        }
      ],
      "source": [
        "import torch.onnx\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    torch.randn(1, 3, 224, 224).to(device),\n",
        "    \"model_original.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=17,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OhxD4SykFKT",
        "outputId": "fefa0844-5983-4454-a61a-410385334996"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 30/30 [00:15<00:00,  1.94it/s, acc=0.8347]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Accuracy: 0.8347\n"
          ]
        }
      ],
      "source": [
        "# --- Step 9: Final Evaluation ---\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loop = tqdm(test_loader, desc='Testing', leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loop.set_postfix({'acc': f\"{test_correct/test_total:.4f}\"})\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoQG-f29ZNuq",
        "outputId": "acbbce35-62c4-401f-a6c2-adba7ba52364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 153 quantizers\n",
            "Smoothed 50 modules\n"
          ]
        }
      ],
      "source": [
        "import modelopt.torch.quantization as mtq\n",
        "import copy\n",
        "\n",
        "# Setup the model\n",
        "model_pre_q = copy.deepcopy(model).to(device)\n",
        "\n",
        "# Select quantization config\n",
        "config = mtq.INT8_SMOOTHQUANT_CFG\n",
        "# config = mtq.INT4_AWQ_CFG\n",
        "# config = mtq.W4A8_AWQ_BETA_CFG\n",
        "\n",
        "# Quantization need calibration data. Setup calibration data loader\n",
        "# An example of creating a calibration data loader looks like the following:\n",
        "data_loader = train_loader\n",
        "\n",
        "\n",
        "# Define forward_loop. Please wrap the data loader in the forward_loop\n",
        "def forward_loop(model):\n",
        "    iterations = 0\n",
        "    for images, labels in data_loader:\n",
        "        images = images.to(device)\n",
        "        model(images)\n",
        "        iterations += 1\n",
        "        if iterations > 13:\n",
        "            break\n",
        "\n",
        "\n",
        "# Quantize the model and perform calibration (PTQ)\n",
        "model_q = mtq.quantize(model_pre_q, config, forward_loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XzOSSO5pwUB",
        "outputId": "8b446bf9-ccf2-4231-ea41-79a07a51063a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dinov2.patch_embed.proj.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=2.6400 calibrator=MaxCalibrator quant)\n",
            "dinov2.patch_embed.proj.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.patch_embed.proj.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0003, 0.0432](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.0201, 1.8003](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0165, 0.4848](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2676, 17.2011](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.1112, 1.5544](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.1143, 5.9699](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0339, 0.4040](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.3167, 3.1884](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0926, 0.5540](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.1470, 4.6218](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0320, 0.1401](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2272, 3.9276](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0771, 1.0664](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.1554, 6.7619](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0417, 0.1228](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2091, 7.1172](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0647, 0.6626](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2116, 4.6014](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0380, 0.1170](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2161, 2.8054](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0622, 0.5304](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2316, 4.0041](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0443, 0.1254](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2275, 2.5707](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0730, 0.3428](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2360, 3.9888](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0451, 0.1475](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2479, 2.4256](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0856, 0.4661](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2286, 2.9831](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0391, 0.2566](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2604, 2.4527](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0755, 0.4333](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2233, 4.1818](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0529, 0.2947](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2594, 2.1369](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0908, 0.4645](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2296, 4.7941](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0489, 0.1592](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2887, 2.5113](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0902, 0.7213](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.qkv.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.qkv.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.attn.qkv.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.3217, 3.8474](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.proj.input_quantizer                                       TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.proj.output_quantizer                                      TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.attn.proj.weight_quantizer                                      TensorQuantizer(8 bit fake axis=0 amax=[0.0646, 0.2326](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc1.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc1.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.mlp.fc1.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.3375, 1.9291](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc2.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc2.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.mlp.fc2.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.0992, 0.7786](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.qkv.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.qkv.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.attn.qkv.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.3852, 5.3878](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.proj.input_quantizer                                       TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.proj.output_quantizer                                      TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.attn.proj.weight_quantizer                                      TensorQuantizer(8 bit fake axis=0 amax=[0.0916, 0.6811](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc1.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc1.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.mlp.fc1.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.3069, 1.5960](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc2.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc2.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.mlp.fc2.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.0799, 1.9985](384) calibrator=MaxCalibrator quant)\n",
            "classifier.0.input_quantizer                                                     TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "classifier.0.output_quantizer                                                    TensorQuantizer(disabled)\n",
            "classifier.0.weight_quantizer                                                    TensorQuantizer(8 bit fake axis=0 amax=[0.5655, 1.3444](512) calibrator=MaxCalibrator quant)\n",
            "classifier.3.input_quantizer                                                     TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "classifier.3.output_quantizer                                                    TensorQuantizer(disabled)\n",
            "classifier.3.weight_quantizer                                                    TensorQuantizer(8 bit fake axis=0 amax=[0.8884, 1.5392](8) calibrator=MaxCalibrator quant)\n",
            "153 TensorQuantizers found in model\n"
          ]
        }
      ],
      "source": [
        "mtq.print_quant_summary(model_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZso4le5p-dE",
        "outputId": "8432fca1-6331-4b4e-f74c-f9946b674cbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing:   0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading extension modelopt_cuda_ext...\n",
            "Loaded extension modelopt_cuda_ext in 0.2 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 30/30 [00:12<00:00,  2.31it/s, acc=0.8297]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Accuracy: 0.8297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluated quantized model\n",
        "model_q = model_q.to(device)\n",
        "model_q.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loop = tqdm(test_loader, desc='Testing', leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_q(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loop.set_postfix({'acc': f\"{test_correct/test_total:.4f}\"})\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=============================================================================================================================================================================================================\n",
              "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Trainable                 Kernel Shape              Mult-Adds\n",
              "=============================================================================================================================================================================================================\n",
              "DINOv2Classifier                                        [128, 3, 224, 224]        [128, 8]                  --                        Partial                   --                        --\n",
              "├─DinoVisionTransformer: 1-1                            [128, 3, 224, 224]        [128, 384]                526,848                   Partial                   --                        --\n",
              "│    └─PatchEmbed: 2-1                                  [128, 3, 224, 224]        [128, 256, 384]           --                        False                     --                        --\n",
              "│    │    └─QuantConv2d: 3-1                            [128, 3, 224, 224]        [128, 384, 16, 16]        226,176                   False                     [14, 14]                  --\n",
              "│    │    │    └─TensorQuantizer: 4-1                   [128, 3, 224, 224]        [128, 3, 224, 224]        --                        --                        --                        --\n",
              "│    │    │    └─TensorQuantizer: 4-2                   [384, 3, 14, 14]          [384, 3, 14, 14]          --                        --                        --                        --\n",
              "│    │    │    └─TensorQuantizer: 4-3                   [128, 384, 16, 16]        [128, 384, 16, 16]        --                        --                        --                        --\n",
              "│    │    └─Identity: 3-2                               [128, 256, 384]           [128, 256, 384]           --                        --                        --                        --\n",
              "│    └─ModuleList: 2-2                                  --                        --                        --                        Partial                   --                        --\n",
              "│    │    └─NestedTensorBlock: 3-3                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-4                         [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-5                   [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-1                  [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-2                      [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-3                  [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-4                      [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-6                        [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-7                         [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-8                               [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-5                  [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-6                         [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-7                      [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-8                  [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-9                      [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-9                        [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-4                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-10                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-11                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-10                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-11                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-12                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-13                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-12                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-13                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-14                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-14                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-15                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-16                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-17                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-18                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-15                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-5                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-16                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-17                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-19                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-20                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-21                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-22                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-18                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-19                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-20                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-23                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-24                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-25                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-26                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-27                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-21                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-6                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-22                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-23                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-28                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-29                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-30                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-31                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-24                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-25                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-26                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-32                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-33                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-34                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-35                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-36                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-27                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-7                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-28                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-29                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-37                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-38                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-39                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-40                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-30                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-31                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-32                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-41                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-42                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-43                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-44                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-45                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-33                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-8                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-34                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-35                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-46                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-47                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-48                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-49                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-36                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-37                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-38                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-50                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-51                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-52                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-53                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-54                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-39                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-9                      [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-40                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-41                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-55                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-56                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-57                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-58                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-42                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-43                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-44                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-59                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-60                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-61                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-62                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-63                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-45                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-10                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-46                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-47                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-64                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-65                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-66                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-67                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-48                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-49                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-50                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-68                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-69                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-70                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-71                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-72                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-51                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-11                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-52                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-53                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-73                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-74                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-75                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-76                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-54                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-55                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-56                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-77                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-78                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-79                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-80                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-81                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-57                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-12                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-58                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-59                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-82                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-83                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-84                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-85                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-60                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-61                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-62                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-86                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-87                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-88                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-89                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-90                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-63                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-13                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-64                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-65                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-91                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-92                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-93                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-94                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-66                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-67                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-68                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-95                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-96                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-97                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-98                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-99                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-69                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-14                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-70                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-71                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-100                [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-101                    [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-102                [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-103                    [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-72                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-73                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-74                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-104                [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-105                       [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-106                    [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-107                [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-108                    [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-75                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    └─LayerNorm: 2-3                                   [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    └─Identity: 2-4                                    [128, 384]                [128, 384]                --                        --                        --                        --\n",
              "├─Sequential: 1-2                                       [128, 384]                [128, 8]                  --                        True                      --                        --\n",
              "│    └─QuantLinear: 2-5                                 [128, 384]                [128, 512]                197,120                   True                      --                        --\n",
              "│    │    └─TensorQuantizer: 3-15                       [128, 384]                [128, 384]                --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-16                       [512, 384]                [512, 384]                --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-17                       [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─GELU: 2-6                                        [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─Dropout: 2-7                                     [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─QuantLinear: 2-8                                 [128, 512]                [128, 8]                  4,104                     True                      --                        --\n",
              "│    │    └─TensorQuantizer: 3-18                       [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-19                       [8, 512]                  [8, 512]                  --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-20                       [128, 8]                  [128, 8]                  --                        --                        --                        --\n",
              "=============================================================================================================================================================================================================\n",
              "Total params: 22,257,800\n",
              "Trainable params: 10,853,384\n",
              "Non-trainable params: 11,404,416\n",
              "Total mult-adds (Units.MEGABYTES): 2.46\n",
              "=============================================================================================================================================================================================================\n",
              "Input size (MB): 77.07\n",
              "Forward/backward pass size (MB): 4951.77\n",
              "Params size (MB): 0.11\n",
              "Estimated Total Size (MB): 5028.95\n",
              "============================================================================================================================================================================================================="
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model_q, (input_shape), device=device, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\", \"kernel_size\", \"mult_adds\"], depth=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UPJkCwaTveqJ"
      },
      "outputs": [],
      "source": [
        "# Save the quantized model\n",
        "import modelopt.torch.opt as mto\n",
        "mto.save(model_q, os.path.join(CKPT_DIR, \"INT8_ptq_model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# del model\n",
        "# del model_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 153 quantizers\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DINOv2Classifier(\n",
              "  (dinov2): DinoVisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): QuantConv2d(\n",
              "        3, 384, kernel_size=(14, 14), stride=(14, 14)\n",
              "        (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=2.6400 calibrator=MaxCalibrator quant)\n",
              "        (output_quantizer): TensorQuantizer(disabled)\n",
              "        (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0003, 0.0432](384) calibrator=MaxCalibrator quant)\n",
              "      )\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0201, 1.8003](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0165, 0.4848](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2676, 17.2011](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.1112, 1.5544](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.1143, 5.9699](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0339, 0.4040](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.3167, 3.1884](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0926, 0.5540](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.1470, 4.6218](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0320, 0.1401](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2272, 3.9276](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0771, 1.0664](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.1554, 6.7619](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0417, 0.1228](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2091, 7.1172](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0647, 0.6626](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (4): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2116, 4.6014](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0380, 0.1170](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2161, 2.8054](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0622, 0.5304](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (5): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2316, 4.0041](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0443, 0.1254](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2275, 2.5707](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0730, 0.3428](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (6): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2360, 3.9888](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0451, 0.1475](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2479, 2.4256](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0856, 0.4661](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (7): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2286, 2.9831](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0391, 0.2566](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2604, 2.4527](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0755, 0.4333](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (8): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2233, 4.1818](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0529, 0.2947](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2594, 2.1369](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0908, 0.4645](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (9): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2296, 4.7941](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0489, 0.1592](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.2887, 2.5113](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0902, 0.7213](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (10): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.3217, 3.8474](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0646, 0.2326](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.3375, 1.9291](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0992, 0.7786](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (11): NestedTensorBlock(\n",
              "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MemEffAttention(\n",
              "          (qkv): QuantLinear(\n",
              "            in_features=384, out_features=1152, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.3852, 5.3878](1152) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): QuantLinear(\n",
              "            in_features=384, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0916, 0.6811](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): LayerScale()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): QuantLinear(\n",
              "            in_features=384, out_features=1536, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.3069, 1.5960](1536) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): QuantLinear(\n",
              "            in_features=1536, out_features=384, bias=True\n",
              "            (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "            (output_quantizer): TensorQuantizer(disabled)\n",
              "            (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.0799, 1.9985](384) calibrator=MaxCalibrator quant)\n",
              "          )\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): LayerScale()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "    (head): Identity()\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): QuantLinear(\n",
              "      in_features=384, out_features=512, bias=True\n",
              "      (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "      (output_quantizer): TensorQuantizer(disabled)\n",
              "      (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.5655, 1.3444](512) calibrator=MaxCalibrator quant)\n",
              "    )\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): QuantLinear(\n",
              "      in_features=512, out_features=8, bias=True\n",
              "      (input_quantizer): TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
              "      (output_quantizer): TensorQuantizer(disabled)\n",
              "      (weight_quantizer): TensorQuantizer(8 bit fake axis=0 amax=[0.8884, 1.5392](8) calibrator=MaxCalibrator quant)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_q = DINOv2Classifier(num_classes=len(classes), model_name=MODEL_NAME, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, blocks=BLOCKS).to(device)\n",
        "mto.restore(model_q, os.path.join(CKPT_DIR, \"INT8_ptq_model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNp9Tyd0qbdI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 [Train]: 100%|██████████| 139/139 [01:09<00:00,  2.01it/s, loss=0.2423, lr=0.000100]\n",
            "Epoch 1/5 [Val]: 100%|██████████| 30/30 [00:13<00:00,  2.30it/s, loss=0.4499, acc=0.8311]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8311\n",
            "\n",
            "Epoch 1 Summary:\n",
            "Train Loss: 0.3383 | Val Loss: 0.7888 | Val Acc: 0.8311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 [Train]: 100%|██████████| 139/139 [01:11<00:00,  1.95it/s, loss=0.4914, lr=0.000090]\n",
            "Epoch 2/5 [Val]: 100%|██████████| 30/30 [00:13<00:00,  2.26it/s, loss=0.5901, acc=0.8229]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "Train Loss: 0.2983 | Val Loss: 0.7650 | Val Acc: 0.8229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 [Train]: 100%|██████████| 139/139 [01:09<00:00,  1.99it/s, loss=0.1599, lr=0.000065]\n",
            "Epoch 3/5 [Val]: 100%|██████████| 30/30 [00:13<00:00,  2.25it/s, loss=0.5219, acc=0.8263]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Summary:\n",
            "Train Loss: 0.2903 | Val Loss: 0.7473 | Val Acc: 0.8263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 [Train]: 100%|██████████| 139/139 [01:09<00:00,  2.00it/s, loss=0.2422, lr=0.000035]\n",
            "Epoch 4/5 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.34it/s, loss=0.7150, acc=0.8379]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8379\n",
            "\n",
            "Epoch 4 Summary:\n",
            "Train Loss: 0.2517 | Val Loss: 0.8621 | Val Acc: 0.8379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 [Train]: 100%|██████████| 139/139 [01:12<00:00,  1.92it/s, loss=0.2404, lr=0.000010]\n",
            "Epoch 5/5 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.34it/s, loss=0.4778, acc=0.8361]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Summary:\n",
            "Train Loss: 0.2568 | Val Loss: 0.7671 | Val Acc: 0.8361\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "LR_HEAD = 1e-4\n",
        "LR_BACKBONE = 3e-5\n",
        "EPOCHS = 5\n",
        "\n",
        "params_ft = [\n",
        "    {\"params\": model_q.classifier.parameters(),                             \"lr\": LR_HEAD},\n",
        "    {\"params\": [p for p in model_q.dinov2.parameters() if p.requires_grad], \"lr\": LR_BACKBONE},\n",
        "]\n",
        "optimizer = optim.AdamW(params_ft)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "train_model(model_q, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=EPOCHS, quantized=True, name=\"INT8_QAT.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_q = DINOv2Classifier(num_classes=len(classes), model_name=MODEL_NAME, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, blocks=BLOCKS).to(device)\n",
        "mto.restore(model_q, os.path.join(CKPT_DIR, \"INT8_QAT.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 30/30 [00:13<00:00,  2.27it/s, acc=0.8334]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Accuracy: 0.8334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluated after qat\n",
        "model_q = model_q.to(device)\n",
        "model_q.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loop = tqdm(test_loader, desc='Testing', leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_q(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loop.set_postfix({'acc': f\"{test_correct/test_total:.4f}\"})\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:72: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert H % patch_H == 0, f\"Input image height {H} is not a multiple of patch height {patch_H}\"\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:73: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert W % patch_W == 0, f\"Input image width {W} is not a multiple of patch width: {patch_W}\"\n",
            "/home/ubuntu/anaconda3/envs/quant/lib/python3.12/site-packages/modelopt/torch/quantization/nn/modules/tensor_quantizer.py:888: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  if not is_torch_export_mode() and len(inputs) == 0:\n",
            "/home/ubuntu/anaconda3/envs/quant/lib/python3.12/site-packages/modelopt/torch/quantization/nn/modules/tensor_quantizer.py:513: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert torch.all(amax >= 0) and not torch.any(torch.isinf(amax)), (\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:183: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if npatch == N and w == h:\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:191: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  M = int(math.sqrt(N))  # Recover the number of patches in each dimension\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:192: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert N == M * M\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:197: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sx = float(w0 + self.interpolate_offset) / M\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:198: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sy = float(h0 + self.interpolate_offset) / M\n",
            "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:209: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (w0, h0) == patch_pos_embed.shape[-2:]\n"
          ]
        }
      ],
      "source": [
        "import torch.onnx\n",
        "\n",
        "torch.onnx.export(\n",
        "    model_q,\n",
        "    torch.randn(1, 3, 224, 224).to(device),\n",
        "    \"modelopt_model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=17,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "mtq.fold_weight(model_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 30/30 [00:12<00:00,  2.32it/s, acc=0.8334]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Accuracy: 0.8334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluated after qat\n",
        "model_q = model_q.to(device)\n",
        "model_q.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loop = tqdm(test_loader, desc='Testing', leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_q(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loop.set_postfix({'acc': f\"{test_correct/test_total:.4f}\"})\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "quant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
