{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1SzyWjzjhqC",
        "outputId": "c4fcb015-4f8d-4f18-8050-b9653b821805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pandas numpy matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5yHq_0kUnEkA"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zruaxu6jlhUQ",
        "outputId": "b487b01d-1bb1-432c-df9e-31f5d2a3623e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Kcl-Jj_x1m",
        "outputId": "6518c1e3-8d9a-4281-d156-fe1eaff9fabd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-10.9.0.34.tar.gz (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12==10.9.0.34 (from tensorrt)\n",
            "  Downloading tensorrt_cu12-10.9.0.34.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12_libs==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt)\n",
            "  Downloading tensorrt_cu12_libs-10.9.0.34.tar.gz (704 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12_bindings==10.9.0.34 (from tensorrt_cu12==10.9.0.34->tensorrt)\n",
            "  Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt_cu12_libs==10.9.0.34->tensorrt_cu12==10.9.0.34->tensorrt) (12.4.127)\n",
            "Downloading tensorrt_cu12_bindings-10.9.0.34-cp311-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt_cu12_libs\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.9.0.34-py2.py3-none-any.whl size=46629 sha256=457c171dc70a9bcda1c58ac5f6157fd898fd7b7cfee6fa31d1f21c468d206b75\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/4d/72/f28cb367f1435d026243047d4f60fde8f1c9cbb06a204f842f\n",
            "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.9.0.34-py2.py3-none-any.whl size=17465 sha256=d19175e2dcf4d3d4dea0b002ef04dc0a4d2afe012411bf07bb5d537e48be3e1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/09/76/6b405075fe4c04097f5713ec0a688df7892aaee823bc141952\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12_libs: filename=tensorrt_cu12_libs-10.9.0.34-py2.py3-none-manylinux_2_28_x86_64.whl size=3103291777 sha256=4a82f0bda2874596f202f6edc8dae99b86a3c4ec2fa142a9c847c4d3a57864a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/d0/06/35d7b3006eead25828debb658da848328ebfd38962a2bcd096\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt_cu12_libs\n",
            "Installing collected packages: tensorrt_cu12_bindings, tensorrt_cu12_libs, tensorrt_cu12, tensorrt\n",
            "Successfully installed tensorrt-10.9.0.34 tensorrt_cu12-10.9.0.34 tensorrt_cu12_bindings-10.9.0.34 tensorrt_cu12_libs-10.9.0.34\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --upgrade tensorrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z6HP5mVj6Kc",
        "outputId": "0dfffdea-1a15-42a2-85d4-00b780977dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading isic-2019.zip to /home/ubuntu\n",
            "100%|██████████████████████████████████████▉| 9.09G/9.10G [01:02<00:00, 176MB/s]\n",
            "100%|███████████████████████████████████████| 9.10G/9.10G [01:02<00:00, 156MB/s]\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "!kaggle datasets download andrewmvd/isic-2019\n",
        "!unzip -q isic-2019.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfBncMXtiC75",
        "outputId": "f944e445-5eab-46d2-c1b6-e93252baf6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.6)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnxsim) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from mako->pycuda) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytools-2025.1.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1-cp311-cp311-linux_x86_64.whl size=660393 sha256=b4da248574d098676932bdd9513ee92a3344e937601d3a7863e5846c5e3e5e1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/7e/6c/d2d1451ea6424cdc3d67b36c16fa7111eafdf2034bc3405666\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, onnx, mako, pycuda, onnxsim\n",
            "Successfully installed mako-1.3.9 onnx-1.17.0 onnxsim-0.4.36 pycuda-2025.1 pytools-2025.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda onnx onnxsim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uvIBd2bsegY4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "vcgQRMYhiHPn",
        "outputId": "618057be-b7fc-4b66-c296-b6f293468008"
      },
      "outputs": [],
      "source": [
        "# TensorRT and CUDA\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit  # Initializes CUDA context\n",
        "\n",
        "import onnx\n",
        "from onnxsim import simplify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OfIr73lyeh3A"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Organize Data ---\n",
        "# Load metadata\n",
        "train_meta = pd.read_csv('ISIC_2019_Training_GroundTruth.csv')\n",
        "train_meta['image'] = train_meta['image'] + '.jpg'\n",
        "\n",
        "# Convert one-hot encoding to class labels\n",
        "classes = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n",
        "train_meta['label'] = train_meta[classes].idxmax(axis=1)\n",
        "label_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "LR_HEAD = 1e-3\n",
        "LR_BACKBONE = 3e-5\n",
        "EPOCHS = 10\n",
        "CKPT_DIR = \"checkpoints\"\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT = 0.2\n",
        "BLOCKS = 6\n",
        "MODEL_NAME = \"dinov2_vits14\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4oYEH2aBj7xD"
      },
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(train_meta, test_size=0.3,\n",
        "                                    stratify=train_meta['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5,\n",
        "                                  stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# --- Step 3: Define Dataset Class ---\n",
        "class ISIC2019Dataset(Dataset):\n",
        "    def __init__(self, df, img_dir='ISIC_2019_Training_Input/ISIC_2019_Training_Input', transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_to_idx = label_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.label_to_idx[self.df.iloc[idx]['label']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fshz4BxQj95f",
        "outputId": "aaedbacb-3906-4c13-9900-a1bfe62a6e6b"
      },
      "outputs": [],
      "source": [
        "# --- Step 4: Data Transforms and Loaders ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create datasets and loaders\n",
        "batch_size = BATCH_SIZE\n",
        "train_dataset = ISIC2019Dataset(train_df, transform=train_transform)\n",
        "val_dataset = ISIC2019Dataset(val_df, transform=val_transform)\n",
        "test_dataset = ISIC2019Dataset(test_df, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zIG4o9GAfVYF"
      },
      "outputs": [],
      "source": [
        "# --- Step 6: Model Setup ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Step 5: Handle Class Imbalance ---\n",
        "train_labels = [label for _, label in train_dataset]\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYjSSyvtj_G6",
        "outputId": "9d037416-477c-4398-feae-883351f863a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        }
      ],
      "source": [
        "class DINOv2Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=8, model_name='dinov2_vitl14', hidden_dim=512, dropout=0.2, blocks=4):\n",
        "        super().__init__()\n",
        "        self.dinov2 = torch.hub.load('facebookresearch/dinov2', model_name)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.dinov2.embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "        for param in self.dinov2.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        for blk in self.dinov2.blocks[-blocks:]:\n",
        "            for p in blk.parameters():\n",
        "                p.requires_grad = True\n",
        "            # if hasattr(blk, 'norm1'):\n",
        "            #     blk.norm1.requires_grad = True\n",
        "        if hasattr(self.dinov2, \"norm\"):\n",
        "            for p in self.dinov2.norm.parameters():\n",
        "                p.requires_grad = True\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get all image features (batch_size, num_patches, embed_dim)\n",
        "        features = self.dinov2(x)\n",
        "\n",
        "        # Handle different output formats:\n",
        "        if features.dim() == 3:  # Standard case with spatial dimensions\n",
        "            cls_token = features[:, 0, :]  # Extract [CLS] token\n",
        "        else:  # Fallback for 2D output\n",
        "            cls_token = features\n",
        "\n",
        "        return self.classifier(cls_token)\n",
        "\n",
        "model = DINOv2Classifier(num_classes=len(classes), model_name=MODEL_NAME, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, blocks=BLOCKS).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "===================================================================================================================================================================================================\n",
              "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Trainable                 Kernel Shape              Mult-Adds\n",
              "===================================================================================================================================================================================================\n",
              "DINOv2Classifier                              [128, 3, 224, 224]        [128, 8]                  --                        Partial                   --                        --\n",
              "├─DinoVisionTransformer: 1-1                  [128, 3, 224, 224]        [128, 384]                526,848                   Partial                   --                        --\n",
              "│    └─PatchEmbed: 2-1                        [128, 3, 224, 224]        [128, 256, 384]           --                        False                     --                        --\n",
              "│    │    └─Conv2d: 3-1                       [128, 3, 224, 224]        [128, 384, 16, 16]        (226,176)                 False                     [14, 14]                  7,411,335,168\n",
              "│    │    └─Identity: 3-2                     [128, 256, 384]           [128, 256, 384]           --                        --                        --                        --\n",
              "│    └─ModuleList: 2-2                        --                        --                        --                        Partial                   --                        --\n",
              "│    │    └─NestedTensorBlock: 3-3            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-1               [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-2         [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-1             [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-2            [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-3             [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-4            [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-3              [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-4               [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-5                     [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-5             [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-6               [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-7            [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-8             [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-9            [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-6              [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-4            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-7               [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-8         [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-10            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-11           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-12            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-13           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-9              [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-10              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-11                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-14            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-15              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-16           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-17            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-18           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-12             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-5            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-13              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-14        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-19            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-20           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-21            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-22           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-15             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-16              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-17                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-23            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-24              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-25           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-26            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-27           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-18             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-6            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-19              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-20        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-28            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-29           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-30            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-31           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-21             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-22              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-23                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-32            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-33              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-34           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-35            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-36           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-24             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-7            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-25              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-26        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-37            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-38           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-39            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-40           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-27             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-28              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-29                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-41            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-42              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-43           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-44            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-45           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-30             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-8            [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-31              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-32        [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-46            [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-47           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-48            [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-49           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-33             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-34              [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-35                    [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─Linear: 5-50            [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-51              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-52           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-53            [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-54           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-36             [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-9            [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-37              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-38        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-55            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-56           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-57            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-58           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-39             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-40              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-41                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-59            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-60              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-61           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-62            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-63           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-42             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-10           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-43              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-44        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-64            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-65           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-66            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-67           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-45             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-46              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-47                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-68            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-69              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-70           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-71            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-72           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-48             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-11           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-49              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-50        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-73            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-74           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-75            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-76           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-51             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-52              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-53                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-77            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-78              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-79           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-80            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-81           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-54             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-12           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-55              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-56        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-82            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-83           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-84            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-85           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-57             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-58              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-59                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-86            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-87              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-88           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-89            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-90           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-60             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-13           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-61              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-62        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-91            [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-92           [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-93            [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-94           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-63             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-64              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-65                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-95            [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-96              [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-97           [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-98            [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-99           [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-66             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-14           [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-67              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-68        [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-100           [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        56,770,560\n",
              "│    │    │    │    └─Dropout: 5-101          [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-102           [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        18,923,520\n",
              "│    │    │    │    └─Dropout: 5-103          [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-69             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-70              [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-71                    [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─Linear: 5-104           [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        75,694,080\n",
              "│    │    │    │    └─GELU: 5-105             [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-106          [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Linear: 5-107           [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        75,546,624\n",
              "│    │    │    │    └─Dropout: 5-108          [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-72             [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    └─LayerNorm: 2-3                         [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    └─Identity: 2-4                          [128, 384]                [128, 384]                --                        --                        --                        --\n",
              "├─Sequential: 1-2                             [128, 384]                [128, 8]                  --                        True                      --                        --\n",
              "│    └─Linear: 2-5                            [128, 384]                [128, 512]                197,120                   True                      --                        25,231,360\n",
              "│    └─GELU: 2-6                              [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─Dropout: 2-7                           [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─Linear: 2-8                            [128, 512]                [128, 8]                  4,104                     True                      --                        525,312\n",
              "===================================================================================================================================================================================================\n",
              "Total params: 22,257,800\n",
              "Trainable params: 10,853,384\n",
              "Non-trainable params: 11,404,416\n",
              "Total mult-adds (Units.GIGABYTES): 10.16\n",
              "===================================================================================================================================================================================================\n",
              "Input size (MB): 77.07\n",
              "Forward/backward pass size (MB): 15967.07\n",
              "Params size (MB): 86.92\n",
              "Estimated Total Size (MB): 16131.06\n",
              "==================================================================================================================================================================================================="
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape = (batch_size, 3, 224, 224)\n",
        "summary(model, (input_shape), device=device, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\", \"kernel_size\", \"mult_adds\"], depth=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xm-d6ERJkBZx"
      },
      "outputs": [],
      "source": [
        "params_ft = [\n",
        "    {\"params\": model.classifier.parameters(),                             \"lr\": LR_HEAD},\n",
        "    {\"params\": [p for p in model.dinov2.parameters() if p.requires_grad], \"lr\": LR_BACKBONE},\n",
        "]\n",
        "optimizer = optim.AdamW(params_ft)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8efHFigmkCZL",
        "outputId": "bb2e3ea8-6d71-4ec3-cd4c-07f417bef887"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 139/139 [01:06<00:00,  2.08it/s, loss=1.1900, lr=0.001000]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.34it/s, loss=1.0838, acc=0.6318]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.6318\n",
            "\n",
            "Epoch 1 Summary:\n",
            "Train Loss: 1.3657 | Val Loss: 1.0076 | Val Acc: 0.6318\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.12it/s, loss=0.8963, lr=0.000976]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.33it/s, loss=0.9023, acc=0.6334]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.6334\n",
            "\n",
            "Epoch 2 Summary:\n",
            "Train Loss: 1.0553 | Val Loss: 0.9738 | Val Acc: 0.6334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 139/139 [01:04<00:00,  2.15it/s, loss=1.4037, lr=0.000905]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.37it/s, loss=0.7272, acc=0.6700]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.6700\n",
            "\n",
            "Epoch 3 Summary:\n",
            "Train Loss: 0.9176 | Val Loss: 0.8869 | Val Acc: 0.6700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.12it/s, loss=1.3067, lr=0.000794]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.42it/s, loss=0.6748, acc=0.7387]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7387\n",
            "\n",
            "Epoch 4 Summary:\n",
            "Train Loss: 0.8463 | Val Loss: 0.8431 | Val Acc: 0.7387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.11it/s, loss=0.3711, lr=0.000655]\n",
            "Epoch 5/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.41it/s, loss=0.6411, acc=0.7671]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7671\n",
            "\n",
            "Epoch 5 Summary:\n",
            "Train Loss: 0.7034 | Val Loss: 0.7613 | Val Acc: 0.7671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.12it/s, loss=0.7833, lr=0.000500]\n",
            "Epoch 6/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.37it/s, loss=0.4404, acc=0.7629]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Summary:\n",
            "Train Loss: 0.6342 | Val Loss: 0.7235 | Val Acc: 0.7629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 139/139 [01:06<00:00,  2.09it/s, loss=0.6169, lr=0.000345]\n",
            "Epoch 7/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.43it/s, loss=0.6022, acc=0.7863]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7863\n",
            "\n",
            "Epoch 7 Summary:\n",
            "Train Loss: 0.5887 | Val Loss: 0.7092 | Val Acc: 0.7863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.12it/s, loss=0.7396, lr=0.000206]\n",
            "Epoch 8/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.37it/s, loss=0.8248, acc=0.8050]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8050\n",
            "\n",
            "Epoch 8 Summary:\n",
            "Train Loss: 0.4818 | Val Loss: 0.7079 | Val Acc: 0.8050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 139/139 [01:05<00:00,  2.12it/s, loss=0.5885, lr=0.000095]\n",
            "Epoch 9/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.43it/s, loss=0.7699, acc=0.8079]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8079\n",
            "\n",
            "Epoch 9 Summary:\n",
            "Train Loss: 0.4478 | Val Loss: 0.6573 | Val Acc: 0.8079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 139/139 [01:06<00:00,  2.10it/s, loss=0.2901, lr=0.000024]\n",
            "Epoch 10/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.42it/s, loss=0.7185, acc=0.8071]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 Summary:\n",
            "Train Loss: 0.4120 | Val Loss: 0.6414 | Val Acc: 0.8071\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = EPOCHS\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=True)\n",
        "\n",
        "    for images, labels in train_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        train_loop.set_postfix({'loss': f\"{loss.item():.4f}\", 'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"})\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loop = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', leave=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_loop.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'acc': f\"{correct/total:.4f}\"\n",
        "            })\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = correct / total\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        directory = os.path.join(CKPT_DIR, f\"best_dinov2_isic2019.pth\")\n",
        "        torch.save(model.state_dict(), directory)\n",
        "        print(f\"\\nNew best model saved with val acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a0i-1GuoD_D",
        "outputId": "fe9dd3ab-c59b-487a-e5ff-e8de71108a63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/home/ubuntu/checkpoints/best_dinov2_isic2019.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OhxD4SykFKT",
        "outputId": "fefa0844-5983-4454-a61a-410385334996"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 30/30 [00:12<00:00,  2.42it/s, acc=0.8042]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Accuracy: 0.8042\n"
          ]
        }
      ],
      "source": [
        "# --- Step 9: Final Evaluation ---\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loop = tqdm(test_loader, desc='Testing', leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loop.set_postfix({'acc': f\"{test_correct/test_total:.4f}\"})\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MDMgqkH_lhsz",
        "outputId": "ead4bb1e-afc9-433e-c965-402fcdecc87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting nvidia-modelopt[all]\n",
            "  Downloading https://pypi.nvidia.com/nvidia-modelopt/nvidia_modelopt-0.25.0-py3-none-manylinux2014_x86_64.whl (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.5/616.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-modelopt-core==0.25.0 (from nvidia-modelopt[all])\n",
            "  Downloading https://pypi.nvidia.com/nvidia-modelopt-core/nvidia_modelopt_core-0.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (3.1.1)\n",
            "Collecting ninja (from nvidia-modelopt[all])\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting numpy<2 (from nvidia-modelopt[all])\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (2.10.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (13.9.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (4.67.1)\n",
            "Collecting cppimport (from nvidia-modelopt[all])\n",
            "  Downloading cppimport-22.8.2.tar.gz (26 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (13.3.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (1.17.0)\n",
            "Collecting onnxconverter-common (from nvidia-modelopt[all])\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting onnxmltools (from nvidia-modelopt[all])\n",
            "  Downloading onnxmltools-1.13.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting onnx-graphsurgeon (from nvidia-modelopt[all])\n",
            "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.6-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime-gpu~=1.20.1 (from nvidia-modelopt[all])\n",
            "  Downloading onnxruntime_gpu-1.20.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting pulp (from nvidia-modelopt[all])\n",
            "  Downloading PuLP-3.0.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (2024.11.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (0.5.3)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (2.6.0+cu124)\n",
            "Collecting torchprofile>=0.0.4 (from nvidia-modelopt[all])\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (0.21.0+cu124)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (12.0.0)\n",
            "Requirement already satisfied: accelerate>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (1.3.0)\n",
            "Requirement already satisfied: datasets>=2.16.1 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (3.4.1)\n",
            "Requirement already satisfied: diffusers>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (0.32.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (0.28.1)\n",
            "Requirement already satisfied: transformers>=4.40.2 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (4.48.3)\n",
            "Requirement already satisfied: peft>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[all]) (0.14.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.27.2->nvidia-modelopt[all]) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.27.2->nvidia-modelopt[all]) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.1->nvidia-modelopt[all]) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (3.11.13)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.27.2->nvidia-modelopt[all]) (8.6.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.27.2->nvidia-modelopt[all]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (4.12.2)\n",
            "Collecting coloredlogs (from onnxruntime-gpu~=1.20.1->nvidia-modelopt[all])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->nvidia-modelopt[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->nvidia-modelopt[all]) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->nvidia-modelopt[all]) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40.2->nvidia-modelopt[all]) (0.21.1)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.11/dist-packages (from cppimport->nvidia-modelopt[all]) (1.3.9)\n",
            "Collecting pybind11 (from cppimport->nvidia-modelopt[all])\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->nvidia-modelopt[all]) (0.8.3)\n",
            "Collecting protobuf (from onnxruntime-gpu~=1.20.1->nvidia-modelopt[all])\n",
            "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->nvidia-modelopt[all]) (12.570.86)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->nvidia-modelopt[all]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->nvidia-modelopt[all]) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.1->nvidia-modelopt[all]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.1->nvidia-modelopt[all]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.1->nvidia-modelopt[all]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.1->nvidia-modelopt[all]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.1->nvidia-modelopt[all]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.1->nvidia-modelopt[all]) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.1->nvidia-modelopt[all]) (1.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt[all]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (2025.1.31)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu~=1.20.1->nvidia-modelopt[all])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers>=0.27.2->nvidia-modelopt[all]) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2->nvidia-modelopt[all]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.1->nvidia-modelopt[all]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.1->nvidia-modelopt[all]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.1->nvidia-modelopt[all]) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.1->nvidia-modelopt[all]) (1.17.0)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.20.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (291.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.5/291.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxmltools-1.13.0-py2.py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PuLP-3.0.2-py3-none-any.whl (17.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cppimport\n",
            "  Building wheel for cppimport (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cppimport: filename=cppimport-22.8.2-py3-none-any.whl size=17699 sha256=f7a6fc12cab46fc31a247ed0734c3c507e13aab40e953d01a049ec206ba4bd6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/74/c3/da98286e4d715a502b9ecbc9d439406597959debe0a0d0a079\n",
            "Successfully built cppimport\n",
            "Installing collected packages: pybind11, pulp, protobuf, nvidia-modelopt-core, numpy, ninja, humanfriendly, cppimport, coloredlogs, onnxruntime-gpu, onnxconverter-common, onnx-graphsurgeon, nvidia-modelopt, onnxmltools, torchprofile\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.2 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 cppimport-22.8.2 humanfriendly-10.0 ninja-1.11.1.3 numpy-1.26.4 nvidia-modelopt-0.25.0 nvidia-modelopt-core-0.25.0 onnx-graphsurgeon-0.5.6 onnxconverter-common-1.14.0 onnxmltools-1.13.0 onnxruntime-gpu-1.20.2 protobuf-3.20.2 pulp-3.0.2 pybind11-2.13.6 torchprofile-0.0.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "cc34ea094c52431da3e03ff42bed3be4",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install \"nvidia-modelopt[all]\" -U --extra-index-url https://pypi.nvidia.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoQG-f29ZNuq",
        "outputId": "acbbce35-62c4-401f-a6c2-adba7ba52364"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/quant/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 153 quantizers\n",
            "Smoothed 50 modules\n"
          ]
        }
      ],
      "source": [
        "import modelopt.torch.quantization as mtq\n",
        "import copy\n",
        "\n",
        "# Setup the model\n",
        "model_pre_q = copy.deepcopy(model).to(device)\n",
        "\n",
        "# Select quantization config\n",
        "config = mtq.INT8_SMOOTHQUANT_CFG\n",
        "\n",
        "# Quantization need calibration data. Setup calibration data loader\n",
        "# An example of creating a calibration data loader looks like the following:\n",
        "data_loader = val_loader\n",
        "\n",
        "\n",
        "# Define forward_loop. Please wrap the data loader in the forward_loop\n",
        "def forward_loop(model):\n",
        "    for images, labels in data_loader:\n",
        "        images = images.to(device)\n",
        "        model(images)\n",
        "\n",
        "\n",
        "# Quantize the model and perform calibration (PTQ)\n",
        "model_q = mtq.quantize(model_pre_q, config, forward_loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XzOSSO5pwUB",
        "outputId": "8b446bf9-ccf2-4231-ea41-79a07a51063a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dinov2.patch_embed.proj.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=2.6400 calibrator=MaxCalibrator quant)\n",
            "dinov2.patch_embed.proj.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.patch_embed.proj.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0003, 0.0432](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.0250, 2.1997](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0259, 0.6135](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2965, 18.6742](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.0.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.0.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.1362, 1.4433](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.1211, 5.9841](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0340, 0.4103](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.3289, 3.2069](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.1.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.1.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.1080, 0.5672](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.1651, 4.7904](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0388, 0.1534](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2182, 3.9617](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.2.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.2.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0824, 1.0684](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.1617, 6.8183](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0394, 0.1306](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2163, 7.1435](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.3.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.3.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0642, 0.6847](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2129, 4.5641](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0464, 0.1264](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2202, 2.8052](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.4.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.4.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0687, 0.5122](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2393, 4.0206](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0491, 0.1409](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2309, 2.5563](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.5.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.5.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0730, 0.3483](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2396, 3.9769](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0423, 0.1514](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2597, 2.4209](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.6.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.6.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0839, 0.4459](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2377, 3.0717](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0467, 0.2633](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2564, 2.4154](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.7.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.7.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0873, 0.4248](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2348, 4.2869](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0510, 0.2770](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2556, 2.1340](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.8.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.8.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0803, 0.4665](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.qkv.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.qkv.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.attn.qkv.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.2389, 4.6818](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.proj.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.attn.proj.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.attn.proj.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.0441, 0.1520](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc1.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc1.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.mlp.fc1.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.2822, 2.4971](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc2.input_quantizer                                          TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.9.mlp.fc2.output_quantizer                                         TensorQuantizer(disabled)\n",
            "dinov2.blocks.9.mlp.fc2.weight_quantizer                                         TensorQuantizer(8 bit fake axis=0 amax=[0.0914, 0.6822](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.qkv.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.qkv.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.attn.qkv.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.3162, 3.6634](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.proj.input_quantizer                                       TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.attn.proj.output_quantizer                                      TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.attn.proj.weight_quantizer                                      TensorQuantizer(8 bit fake axis=0 amax=[0.0651, 0.2643](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc1.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc1.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.mlp.fc1.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.3504, 2.0116](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc2.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.10.mlp.fc2.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.10.mlp.fc2.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.0916, 0.6838](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.qkv.input_quantizer                                        TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.qkv.output_quantizer                                       TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.attn.qkv.weight_quantizer                                       TensorQuantizer(8 bit fake axis=0 amax=[0.3701, 5.3610](1152) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.proj.input_quantizer                                       TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.attn.proj.output_quantizer                                      TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.attn.proj.weight_quantizer                                      TensorQuantizer(8 bit fake axis=0 amax=[0.0950, 0.8191](384) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc1.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc1.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.mlp.fc1.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.3069, 1.4580](1536) calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc2.input_quantizer                                         TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "dinov2.blocks.11.mlp.fc2.output_quantizer                                        TensorQuantizer(disabled)\n",
            "dinov2.blocks.11.mlp.fc2.weight_quantizer                                        TensorQuantizer(8 bit fake axis=0 amax=[0.0762, 1.7988](384) calibrator=MaxCalibrator quant)\n",
            "classifier.0.input_quantizer                                                     TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "classifier.0.output_quantizer                                                    TensorQuantizer(disabled)\n",
            "classifier.0.weight_quantizer                                                    TensorQuantizer(8 bit fake axis=0 amax=[0.4895, 1.1994](512) calibrator=MaxCalibrator quant)\n",
            "classifier.3.input_quantizer                                                     TensorQuantizer(8 bit fake per-tensor amax=1.0000 pre_quant_scale calibrator=MaxCalibrator quant)\n",
            "classifier.3.output_quantizer                                                    TensorQuantizer(disabled)\n",
            "classifier.3.weight_quantizer                                                    TensorQuantizer(8 bit fake axis=0 amax=[0.7529, 1.3993](8) calibrator=MaxCalibrator quant)\n",
            "153 TensorQuantizers found in model\n"
          ]
        }
      ],
      "source": [
        "mtq.print_quant_summary(model_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZso4le5p-dE",
        "outputId": "8432fca1-6331-4b4e-f74c-f9946b674cbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing:   0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading extension modelopt_cuda_ext...\n",
            "Loaded extension modelopt_cuda_ext in 56.1 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 30/30 [01:07<00:00,  2.26s/it, acc=0.7997]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Accuracy: 0.7997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluated quantized model\n",
        "model_q = model_q.to(device)\n",
        "model_q.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loop = tqdm(test_loader, desc='Testing', leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_q(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loop.set_postfix({'acc': f\"{test_correct/test_total:.4f}\"})\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=============================================================================================================================================================================================================\n",
              "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Trainable                 Kernel Shape              Mult-Adds\n",
              "=============================================================================================================================================================================================================\n",
              "DINOv2Classifier                                        [128, 3, 224, 224]        [128, 8]                  --                        Partial                   --                        --\n",
              "├─DinoVisionTransformer: 1-1                            [128, 3, 224, 224]        [128, 384]                526,848                   Partial                   --                        --\n",
              "│    └─PatchEmbed: 2-1                                  [128, 3, 224, 224]        [128, 256, 384]           --                        False                     --                        --\n",
              "│    │    └─QuantConv2d: 3-1                            [128, 3, 224, 224]        [128, 384, 16, 16]        226,176                   False                     [14, 14]                  --\n",
              "│    │    │    └─TensorQuantizer: 4-1                   [128, 3, 224, 224]        [128, 3, 224, 224]        --                        --                        --                        --\n",
              "│    │    │    └─TensorQuantizer: 4-2                   [384, 3, 14, 14]          [384, 3, 14, 14]          --                        --                        --                        --\n",
              "│    │    │    └─TensorQuantizer: 4-3                   [128, 384, 16, 16]        [128, 384, 16, 16]        --                        --                        --                        --\n",
              "│    │    └─Identity: 3-2                               [128, 256, 384]           [128, 256, 384]           --                        --                        --                        --\n",
              "│    └─ModuleList: 2-2                                  --                        --                        --                        Partial                   --                        --\n",
              "│    │    └─NestedTensorBlock: 3-3                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-4                         [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-5                   [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-1                  [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-2                      [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-3                  [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-4                      [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-6                        [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-7                         [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-8                               [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-5                  [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-6                         [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-7                      [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-8                  [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-9                      [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-9                        [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-4                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-10                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-11                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-10                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-11                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-12                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-13                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-12                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-13                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-14                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-14                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-15                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-16                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-17                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-18                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-15                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-5                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-16                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-17                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-19                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-20                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-21                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-22                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-18                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-19                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-20                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-23                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-24                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-25                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-26                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-27                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-21                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-6                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-22                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-23                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-28                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-29                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-30                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-31                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-24                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-25                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-26                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-32                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-33                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-34                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-35                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-36                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-27                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-7                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-28                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-29                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-37                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-38                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-39                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-40                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-30                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-31                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-32                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-41                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-42                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-43                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-44                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-45                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-33                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-8                      [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-34                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-35                  [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-46                 [128, 257, 384]           [128, 257, 1152]          (443,520)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-47                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-48                 [128, 257, 384]           [128, 257, 384]           (147,840)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-49                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-36                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    │    └─LayerNorm: 4-37                        [128, 257, 384]           [128, 257, 384]           (768)                     False                     --                        98,304\n",
              "│    │    │    └─Mlp: 4-38                              [128, 257, 384]           [128, 257, 384]           --                        False                     --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-50                 [128, 257, 384]           [128, 257, 1536]          (591,360)                 False                     --                        0\n",
              "│    │    │    │    └─GELU: 5-51                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-52                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-53                 [128, 257, 1536]          [128, 257, 384]           (590,208)                 False                     --                        0\n",
              "│    │    │    │    └─Dropout: 5-54                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-39                       [128, 257, 384]           [128, 257, 384]           (384)                     False                     --                        --\n",
              "│    │    └─NestedTensorBlock: 3-9                      [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-40                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-41                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-55                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-56                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-57                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-58                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-42                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-43                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-44                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-59                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-60                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-61                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-62                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-63                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-45                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-10                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-46                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-47                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-64                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-65                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-66                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-67                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-48                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-49                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-50                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-68                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-69                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-70                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-71                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-72                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-51                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-11                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-52                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-53                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-73                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-74                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-75                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-76                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-54                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-55                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-56                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-77                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-78                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-79                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-80                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-81                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-57                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-12                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-58                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-59                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-82                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-83                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-84                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-85                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-60                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-61                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-62                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-86                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-87                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-88                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-89                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-90                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-63                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-13                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-64                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-65                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-91                 [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-92                     [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-93                 [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-94                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-66                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-67                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-68                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-95                 [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-96                        [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-97                     [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-98                 [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-99                     [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-69                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    └─NestedTensorBlock: 3-14                     [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-70                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─MemEffAttention: 4-71                  [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-100                [128, 257, 384]           [128, 257, 1152]          443,520                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-101                    [128, 6, 257, 257]        [128, 6, 257, 257]        --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-102                [128, 257, 384]           [128, 257, 384]           147,840                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-103                    [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-72                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    │    │    └─LayerNorm: 4-73                        [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    │    │    └─Mlp: 4-74                              [128, 257, 384]           [128, 257, 384]           --                        True                      --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-104                [128, 257, 384]           [128, 257, 1536]          591,360                   True                      --                        0\n",
              "│    │    │    │    └─GELU: 5-105                       [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─Dropout: 5-106                    [128, 257, 1536]          [128, 257, 1536]          --                        --                        --                        --\n",
              "│    │    │    │    └─QuantLinear: 5-107                [128, 257, 1536]          [128, 257, 384]           590,208                   True                      --                        0\n",
              "│    │    │    │    └─Dropout: 5-108                    [128, 257, 384]           [128, 257, 384]           --                        --                        --                        --\n",
              "│    │    │    └─LayerScale: 4-75                       [128, 257, 384]           [128, 257, 384]           384                       True                      --                        --\n",
              "│    └─LayerNorm: 2-3                                   [128, 257, 384]           [128, 257, 384]           768                       True                      --                        98,304\n",
              "│    └─Identity: 2-4                                    [128, 384]                [128, 384]                --                        --                        --                        --\n",
              "├─Sequential: 1-2                                       [128, 384]                [128, 8]                  --                        True                      --                        --\n",
              "│    └─QuantLinear: 2-5                                 [128, 384]                [128, 512]                197,120                   True                      --                        --\n",
              "│    │    └─TensorQuantizer: 3-15                       [128, 384]                [128, 384]                --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-16                       [512, 384]                [512, 384]                --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-17                       [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─GELU: 2-6                                        [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─Dropout: 2-7                                     [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    └─QuantLinear: 2-8                                 [128, 512]                [128, 8]                  4,104                     True                      --                        --\n",
              "│    │    └─TensorQuantizer: 3-18                       [128, 512]                [128, 512]                --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-19                       [8, 512]                  [8, 512]                  --                        --                        --                        --\n",
              "│    │    └─TensorQuantizer: 3-20                       [128, 8]                  [128, 8]                  --                        --                        --                        --\n",
              "=============================================================================================================================================================================================================\n",
              "Total params: 22,257,800\n",
              "Trainable params: 10,853,384\n",
              "Non-trainable params: 11,404,416\n",
              "Total mult-adds (Units.MEGABYTES): 2.46\n",
              "=============================================================================================================================================================================================================\n",
              "Input size (MB): 77.07\n",
              "Forward/backward pass size (MB): 4951.77\n",
              "Params size (MB): 0.11\n",
              "Estimated Total Size (MB): 5028.95\n",
              "============================================================================================================================================================================================================="
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model_q, (input_shape), device=device, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\", \"kernel_size\", \"mult_adds\"], depth=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPJkCwaTveqJ"
      },
      "outputs": [],
      "source": [
        "# Save the quantized model\n",
        "import modelopt.torch.opt as mto\n",
        "mto.save(model_q, \"modelopt_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Model already has modelopt state!",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodelopt_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/quant/lib/python3.12/site-packages/modelopt/torch/opt/conversion.py:595\u001b[39m, in \u001b[36mrestore\u001b[39m\u001b[34m(model, f, **kwargs)\u001b[39m\n\u001b[32m    592\u001b[39m objs = torch.load(f, **kwargs)\n\u001b[32m    594\u001b[39m \u001b[38;5;66;03m# restore model architecture\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m model_restored = \u001b[43mrestore_from_modelopt_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodelopt_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# load weights from checkpoint\u001b[39;00m\n\u001b[32m    598\u001b[39m model_restored.load_state_dict(objs[\u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/quant/lib/python3.12/site-packages/modelopt/torch/opt/conversion.py:547\u001b[39m, in \u001b[36mrestore_from_modelopt_state\u001b[39m\u001b[34m(model, modelopt_state)\u001b[39m\n\u001b[32m    544\u001b[39m model = model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, nn.Module) \u001b[38;5;28;01melse\u001b[39;00m ModelLikeModule(model)\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m# initialize state manager and load state\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m manager = \u001b[43mModeloptStateManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m manager.load_state_dict(\n\u001b[32m    549\u001b[39m     modelopt_state[\u001b[33m\"\u001b[39m\u001b[33mmodelopt_state_dict\u001b[39m\u001b[33m\"\u001b[39m], modelopt_state[\u001b[33m\"\u001b[39m\u001b[33mmodelopt_version\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    550\u001b[39m )\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# apply restore entrypoints for each of the modes\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/quant/lib/python3.12/site-packages/modelopt/torch/opt/conversion.py:83\u001b[39m, in \u001b[36mModeloptStateManager.__init__\u001b[39m\u001b[34m(self, model, init_state)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# initialize modelopt state if desired. Note that by default we don't do that to avoid\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# accidentally modifying a user-provided model.\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m init_state:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;28mself\u001b[39m._state_key), \u001b[33m\"\u001b[39m\u001b[33mModel already has modelopt state!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(model, \u001b[38;5;28mself\u001b[39m._state_key, [])\n\u001b[32m     85\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(model, \u001b[38;5;28mself\u001b[39m._state_version_key, [__version__])\n",
            "\u001b[31mAssertionError\u001b[39m: Model already has modelopt state!"
          ]
        }
      ],
      "source": [
        "# mto.restore(model_q, \"modelopt_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YNp9Tyd0qbdI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.02it/s, loss=0.7214, lr=0.001000]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.32it/s, loss=1.0068, acc=0.7474]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7474\n",
            "\n",
            "Epoch 1 Summary:\n",
            "Train Loss: 0.5064 | Val Loss: 0.7491 | Val Acc: 0.7474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 139/139 [01:07<00:00,  2.05it/s, loss=0.3442, lr=0.000976]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.33it/s, loss=0.8146, acc=0.7416]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "Train Loss: 0.4940 | Val Loss: 0.6771 | Val Acc: 0.7416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 139/139 [01:09<00:00,  2.01it/s, loss=0.7879, lr=0.000905]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.32it/s, loss=0.5038, acc=0.7808]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.7808\n",
            "\n",
            "Epoch 3 Summary:\n",
            "Train Loss: 0.4576 | Val Loss: 0.6818 | Val Acc: 0.7808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 139/139 [01:07<00:00,  2.06it/s, loss=0.4817, lr=0.000794]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 30/30 [00:13<00:00,  2.27it/s, loss=0.4914, acc=0.8005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8005\n",
            "\n",
            "Epoch 4 Summary:\n",
            "Train Loss: 0.4179 | Val Loss: 0.5941 | Val Acc: 0.8005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 139/139 [01:06<00:00,  2.08it/s, loss=0.2305, lr=0.000655]\n",
            "Epoch 5/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.39it/s, loss=0.6559, acc=0.8092]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8092\n",
            "\n",
            "Epoch 5 Summary:\n",
            "Train Loss: 0.3942 | Val Loss: 0.6741 | Val Acc: 0.8092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.03it/s, loss=0.3767, lr=0.000500]\n",
            "Epoch 6/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.38it/s, loss=0.5821, acc=0.8137]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8137\n",
            "\n",
            "Epoch 6 Summary:\n",
            "Train Loss: 0.3659 | Val Loss: 0.5921 | Val Acc: 0.8137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.04it/s, loss=1.3843, lr=0.000345]\n",
            "Epoch 7/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.39it/s, loss=0.6174, acc=0.8297]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8297\n",
            "\n",
            "Epoch 7 Summary:\n",
            "Train Loss: 0.3518 | Val Loss: 0.6148 | Val Acc: 0.8297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 139/139 [01:09<00:00,  2.01it/s, loss=0.4447, lr=0.000206]\n",
            "Epoch 8/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.35it/s, loss=0.6632, acc=0.8321]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8321\n",
            "\n",
            "Epoch 8 Summary:\n",
            "Train Loss: 0.3340 | Val Loss: 0.6116 | Val Acc: 0.8321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 139/139 [01:07<00:00,  2.05it/s, loss=0.2018, lr=0.000095]\n",
            "Epoch 9/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.34it/s, loss=0.7680, acc=0.8234]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 Summary:\n",
            "Train Loss: 0.3058 | Val Loss: 0.6379 | Val Acc: 0.8234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 139/139 [01:08<00:00,  2.02it/s, loss=0.3492, lr=0.000024]\n",
            "Epoch 10/10 [Val]: 100%|██████████| 30/30 [00:12<00:00,  2.36it/s, loss=0.7216, acc=0.8350]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best model saved with val acc: 0.8350\n",
            "\n",
            "Epoch 10 Summary:\n",
            "Train Loss: 0.3014 | Val Loss: 0.6344 | Val Acc: 0.8350\n"
          ]
        }
      ],
      "source": [
        "params_ft = [\n",
        "    {\"params\": model_q.classifier.parameters(),                             \"lr\": LR_HEAD},\n",
        "    {\"params\": [p for p in model_q.dinov2.parameters() if p.requires_grad], \"lr\": LR_BACKBONE},\n",
        "]\n",
        "optimizer = optim.AdamW(params_ft)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "num_epochs = EPOCHS\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model_q.train()\n",
        "    train_loss = 0.0\n",
        "    train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=True)\n",
        "\n",
        "    for images, labels in train_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_q(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        train_loop.set_postfix({'loss': f\"{loss.item():.4f}\", 'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"})\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model_q.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loop = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', leave=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model_q(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_loop.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'acc': f\"{correct/total:.4f}\"\n",
        "            })\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = correct / total\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        directory = os.path.join(CKPT_DIR, \"best_quantized_model.pth\")\n",
        "        mto.save(model_q, directory)\n",
        "        print(f\"\\nNew best model saved with val acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 30/30 [00:12<00:00,  2.35it/s, acc=0.8203]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Accuracy: 0.8203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluated after qat\n",
        "model_q = model_q.to(device)\n",
        "model_q.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loop = tqdm(test_loader, desc='Testing', leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_q(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loop.set_postfix({'acc': f\"{test_correct/test_total:.4f}\"})\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "quant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
