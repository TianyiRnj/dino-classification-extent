{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed --quiet\n",
        "!pip install mpi4py --quiet"
      ],
      "metadata": {
        "id": "9oOgudaHNOmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IArHBmtRB-w8",
        "outputId": "ea14c7b2-9c2f-4317-81ea-0e6550b8d5ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-25 12:39:31,260] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.jit import trace\n",
        "from tqdm import tqdm\n",
        "import deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Use the same Kaggle code from HW1P2\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    # Put your kaggle username & key here\n",
        "    f.write('{\"username\":\"tianyir\",\"key\":\"e9c337dcee4fb617dcc1607ed3094a25\"}')\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "qn3wu7ObCrB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"andrewmvd/isic-2019\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1U4QBZXPvVQ",
        "outputId": "5961a69c-70a8-42a5-998b-29988b62f97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/isic-2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuPblZaNB-w-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 4\n",
        "LR_HEAD = 1e-3\n",
        "LR_BACKBONE = 1e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "EPOCHS_LP = 10\n",
        "EPOCHS_FT = 8\n",
        "UNFREEZE_BLOCKS = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Organize Data ---\n",
        "# Load metadata\n",
        "img_dir  = \"/root/.cache/kagglehub/datasets/andrewmvd/isic-2019/versions/1/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\n",
        "path = '/root/.cache/kagglehub/datasets/andrewmvd/isic-2019/versions/1'\n",
        "df = pd.read_csv(f'{path}/ISIC_2019_Training_GroundTruth.csv')\n",
        "df['image_id'] = df['image'] + '.jpg'\n",
        "\n",
        "# Convert one-hot encoding to class labels\n",
        "classes = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n",
        "df['dx'] = df[classes].idxmax(axis=1)\n",
        "label_to_idx = {cls: idx for idx, cls in enumerate(classes)}"
      ],
      "metadata": {
        "id": "vhXCDX_1T_VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.2,\n",
        "                                    stratify=df['dx'], random_state=42)\n",
        "\n",
        "# --- Step 3: Define Dataset Class ---\n",
        "class ISIC2019Dataset(Dataset):\n",
        "    def __init__(self, df, img_dir='ISIC_2019_Training_Input/ISIC_2019_Training_Input', transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_to_idx = label_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.label_to_idx[self.df.iloc[idx]['label']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "YXEHfshBV2vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Yea60sB-w_"
      },
      "outputs": [],
      "source": [
        "# meta_csv = \"/root/.cache/kagglehub/datasets/andrewmvd/isic-2019/versions/1/ISIC_2019_Training_Metadata.csv\"\n",
        "# img_dir  = \"/root/.cache/kagglehub/datasets/andrewmvd/isic-2019/versions/1\"\n",
        "\n",
        "# df = pd.read_csv(meta_csv)\n",
        "# df['image_id'] = df['image_id'].apply(lambda x: f\"{x}.jpg\")\n",
        "# df = df[df['dx'].notna()]\n",
        "\n",
        "# train_df, val_df = train_test_split(\n",
        "#     df, test_size=0.2, stratify=df['dx'], random_state=42\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERnByLixB-w_"
      },
      "outputs": [],
      "source": [
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkbySKy8B-xA"
      },
      "outputs": [],
      "source": [
        "class HAM10000Dataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(self.df['dx'].unique())\n",
        "        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        path = os.path.join(self.img_dir, row['image_id'])\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.class_to_idx[row['dx']]\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXJ3NPrCB-xB",
        "outputId": "39819e6e-030e-4300-d6fb-e42f1a88c96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "t_train = HAM10000Dataset(train_df, img_dir, transform=train_tfms)\n",
        "t_val   = HAM10000Dataset(val_df,   img_dir, transform=val_tfms)\n",
        "train_loader = DataLoader(t_train, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(t_val,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "NUM_CLASSES = len(t_train.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8p_STH3B-xC",
        "outputId": "b56cef28-bacb-417e-9226-ec1bb1887387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        }
      ],
      "source": [
        "backbone = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14\").to(device)\n",
        "feat_dim = backbone.embed_dim\n",
        "num_blocks = len(backbone.blocks)\n",
        "\n",
        "head = nn.Linear(feat_dim, NUM_CLASSES).to(device)\n",
        "\n",
        "class DinoClassifier(nn.Module):\n",
        "    def __init__(self, backbone, head):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        feats = self.backbone.norm(feats)\n",
        "        return self.head(feats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IzPTDk2B-xC"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for x, y in tqdm(loader, desc=\"Eval\", leave=False):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = model(x).argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "def measure_inference_time(model, device, runs=100):\n",
        "    model.eval()\n",
        "    example = torch.randn(1,3,224,224).to(device)\n",
        "    for _ in range(10): _ = model(example)\n",
        "    torch.cuda.synchronize()\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    for _ in range(runs): _ = model(example)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_8pv4iyvB-xD",
        "outputId": "49babad6-3c48-4191-f516-22207a975fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Linear Probing Training ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 — loss: 0.6867, val_acc: 77.1660%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 — loss: 0.6676, val_acc: 74.5214%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 — loss: 0.6583, val_acc: 76.5739%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 — loss: 0.6584, val_acc: 76.8305%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 — loss: 0.6540, val_acc: 76.8502%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 — loss: 0.6519, val_acc: 76.6923%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 — loss: 0.6386, val_acc: 77.2647%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 — loss: 0.6395, val_acc: 77.5015%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 — loss: 0.6427, val_acc: 76.6923%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 — loss: 0.6276, val_acc: 77.5607%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "# ========== Linear Probing ==========\n",
        "for p in backbone.parameters(): p.requires_grad = False\n",
        "for p in head.parameters():    p.requires_grad = True\n",
        "model_lp = DinoClassifier(backbone, head).to(device)\n",
        "optimizer_lp = optim.AdamW(head.parameters(), lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"=== Linear Probing Training ===\")\n",
        "for epoch in range(EPOCHS_LP):\n",
        "    loss = train_epoch(model_lp, train_loader, optimizer_lp, criterion, device)\n",
        "    acc  = evaluate(model_lp, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS_LP} — loss: {loss:.4f}, val_acc: {acc:.4%}\")\n",
        "acc_lp  = evaluate(model_lp, val_loader, device)\n",
        "time_lp = measure_inference_time(model_lp, device)\n",
        "torch.jit.trace(model_lp.eval(), torch.randn(1,3,224,224).to(device)).save(\"model_lp.ts\")\n",
        "size_lp = os.path.getsize(\"model_lp.ts\")/1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkDx0Fw3B-xE",
        "outputId": "84cbdad0-01d0-491a-ca45-3dacf4775927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Partial Fine-tuning ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8 — loss: 0.7303, val_acc: 78.1330%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/8 — loss: 0.6468, val_acc: 77.3239%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/8 — loss: 0.5717, val_acc: 77.3041%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/8 — loss: 0.5396, val_acc: 81.2710%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/8 — loss: 0.4784, val_acc: 81.8038%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/8 — loss: 0.4516, val_acc: 77.6791%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/8 — loss: 0.4162, val_acc: 83.5800%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/8 — loss: 0.3763, val_acc: 83.3827%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "# ========== Partial Fine-tuning ==========\n",
        "for p in backbone.parameters():  p.requires_grad = False\n",
        "for blk in backbone.blocks[-UNFREEZE_BLOCKS:]:\n",
        "    for p in blk.parameters(): p.requires_grad = True\n",
        "for p in head.parameters(): p.requires_grad = True\n",
        "params_ft = [\n",
        "    {\"params\": head.parameters(), \"lr\": LR_HEAD},\n",
        "    {\"params\": [p for p in backbone.parameters() if p.requires_grad], \"lr\": LR_BACKBONE},\n",
        "]\n",
        "optimizer_ft = optim.AdamW(params_ft, weight_decay=WEIGHT_DECAY)\n",
        "print(\"=== Partial Fine-tuning ===\")\n",
        "for epoch in range(EPOCHS_FT):\n",
        "    loss = train_epoch(model_lp, train_loader, optimizer_ft, criterion, device)\n",
        "    acc  = evaluate(model_lp, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS_FT} — loss: {loss:.4f}, val_acc: {acc:.4%}\")\n",
        "acc_ft  = evaluate(model_lp, val_loader, device)\n",
        "time_ft = measure_inference_time(model_lp, device)\n",
        "ts_mod = torch.jit.trace(model_lp.eval(), torch.randn(1,3,224,224).to(device))\n",
        "ts_mod.save(\"model_ft.ts\")\n",
        "size_ft = os.path.getsize(\"model_ft.ts\")/1e6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_half(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device).half()       # 关键：输入也转为 half\n",
        "        y = y.to(device)\n",
        "        pred = model(x).argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "def measure_inference_time_half(model, device, runs=100):\n",
        "    model.eval()\n",
        "    example = torch.randn(1,3,224,224, device=device, dtype=torch.half)\n",
        "    for _ in range(10): _ = model(example)\n",
        "    torch.cuda.synchronize()\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    for _ in range(runs): _ = model(example)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / runs"
      ],
      "metadata": {
        "id": "zmERQR_TuUr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_model_16qt = deepspeed.init_inference(\n",
        "    model_lp,\n",
        "    dtype=torch.float16,             # 量化到 float16\n",
        "    replace_method='auto',           # 自动定位哪些层可量化\n",
        "    replace_with_kernel_inject=True  # 注入高性能量化内核\n",
        ")\n",
        "\n",
        "# 3) 移到 GPU\n",
        "ds_model_16qt.cuda().eval()\n",
        "\n",
        "# 4) 直接用 ds_model_16qt 做推理\n",
        "example = torch.randn(1, 3, 224, 224, device=device, dtype=torch.half)\n",
        "ds_mod_16qt_trt = torch.jit.trace(ds_model_16qt, example)\n",
        "\n",
        "torch.jit.save(ds_mod_16qt_trt, \"model_ds_float16.ts\")\n",
        "\n",
        "\n",
        "ds_acc_16qt = evaluate_half(ds_model_16qt, val_loader, device)\n",
        "\n",
        "def measure_inference_time_half(model, device, runs=100):\n",
        "    model.eval()\n",
        "    example = torch.randn(1,3,224,224, device=device, dtype=torch.half)\n",
        "    for _ in range(10): _ = model(example)\n",
        "    torch.cuda.synchronize()\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    for _ in range(runs): _ = model(example)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / runs\n",
        "\n",
        "ds_time_16qt = measure_inference_time_half(ds_model_16qt, device)\n",
        "ds_size_16qt = os.path.getsize(\"model_ds_float16.ts\")/1e6\n",
        "\n",
        "print(\"\\n=== Deepspeed float16 Quantization Results ===\")\n",
        "print(f\"Accuracy: {ds_acc_16qt:.4%}, Inference Time: {ds_time_16qt:.2f}ms, Model Size: {ds_size_16qt:.2f}MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwXWONetzWnd",
        "outputId": "8bff80a2-e87c-4b85-9808-709e4be89cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-25 15:22:52,430] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n",
            "[2025-04-25 15:22:52,465] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2025-04-25 15:22:52,467] [INFO] [logging.py:107:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:1304: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Tensor-likes are not close!\n",
            "\n",
            "Mismatched elements: 5 / 8 (62.5%)\n",
            "Greatest absolute difference: 0.0078125 at index (0, 3) (up to 1e-05 allowed)\n",
            "Greatest relative difference: 0.0015552099533437014 at index (0, 3) (up to 1e-05 allowed)\n",
            "  _check_trace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Deepspeed float16 Quantization Results ===\n",
            "Accuracy: 83.3432%, Inference Time: 9.84ms, Model Size: 173.63MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "# 1) Freeze entire backbone\n",
        "for p in backbone.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 2) Unfreeze last UNFREEZE_BLOCKS transformer blocks\n",
        "for blk in backbone.blocks[-UNFREEZE_BLOCKS:]:\n",
        "    for p in blk.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "# 3) Unfreeze the classification head\n",
        "for p in head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "params_ft = [\n",
        "    {\"params\": head.parameters(),                              \"lr\": LR_HEAD},\n",
        "    {\"params\": [p for p in backbone.parameters() if p.requires_grad], \"lr\": LR_BACKBONE},\n",
        "]\n",
        "optimizer_ft = torch.optim.AdamW(params_ft, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "model_engine = None\n",
        "model_engine, optimizer_engine, _, _ = deepspeed.initialize(\n",
        "    args=None,\n",
        "    model= model_lp,                             # your nn.Module wrapper of backbone+head\n",
        "    model_parameters=params_ft,\n",
        "    config_params={\n",
        "        \"train_batch_size\": BATCH_SIZE,\n",
        "        \"fp16\": {\"enabled\": True},               # if you want mixed-precision\n",
        "        \"optimizer\": {\n",
        "            \"type\": \"AdamW\",\n",
        "            \"params\": {\"weight_decay\": WEIGHT_DECAY}\n",
        "        },\n",
        "        \"timers\": {\"enabled\": False},\n",
        "    }\n",
        ")\n",
        "for epoch in range(EPOCHS_FT):\n",
        "    model_engine.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device).half(), y.to(device)\n",
        "        model_engine.zero_grad()\n",
        "        logits = model_engine(x)            # same as model_lp(x) under the hood\n",
        "        loss = criterion(logits, y)\n",
        "        model_engine.backward(loss)         # DeepSpeed-aware backward\n",
        "        model_engine.step()                 # DeepSpeed step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    val_acc = evaluate_half(model_engine, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS_FT} — loss: {total_loss/len(train_loader.dataset):.4f}, val_acc: {val_acc:.4%}\")\n",
        "\n",
        "model_engine.eval()\n",
        "# Example input must match the dtype you trained with (e.g., fp16 if fp16 enabled)\n",
        "example = torch.randn(1, 3, 224, 224, device=device, dtype=torch.half)\n",
        "ts_mod = torch.jit.trace(model_engine, example)  # ScriptModule generation :contentReference[oaicite:3]{index=3}\n",
        "ts_mod.save(\"model_ft.ts\")\n",
        "\n",
        "ds_acc_16qt_ft = evaluate_half(model_engine, val_loader, device)\n",
        "ds_time_16qt_ft = measure_inference_time_half(model_engine, device)\n",
        "ds_size_16qt_ft = os.path.getsize(\"model_ft.ts\") / 1e6\n",
        "\n",
        "print(\"\\n=== Partial Fine-Tuning Results ===\")\n",
        "print(f\"Accuracy:      {ds_acc_16qt_ft:.4%}\")\n",
        "print(f\"Latency:       {ds_time_16qt_ft:.2f} ms\")\n",
        "print(f\"Model Size:    {ds_size_16qt_ft:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6VYpGFYHBcE",
        "outputId": "37a04050-adb3-4861-efaf-40bf282e3a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-04-25 15:24:19,518] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n",
            "[2025-04-25 15:24:19,519] [INFO] [comm.py:669:init_distributed] cdb=None\n",
            "[2025-04-25 15:24:19,520] [INFO] [comm.py:684:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
            "[2025-04-25 15:24:20,253] [INFO] [comm.py:739:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n",
            "[2025-04-25 15:24:20,254] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2025-04-25 15:24:20,311] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 1\n",
            "[2025-04-25 15:24:20,724] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/fused_adam/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to load fused_adam op: 0.15436458587646484 seconds\n",
            "[2025-04-25 15:24:20,886] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2025-04-25 15:24:20,889] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2025-04-25 15:24:20,891] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2025-04-25 15:24:20,892] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
            "[2025-04-25 15:24:20,903] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer\n",
            "[2025-04-25 15:24:20,903] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
            "[2025-04-25 15:24:20,905] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2025-04-25 15:24:20,906] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
            "[2025-04-25 15:24:20,908] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
            "[2025-04-25 15:24:20,910] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2025-04-25 15:24:20,911] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
            "[2025-04-25 15:24:20,912] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
            "[2025-04-25 15:24:20,913] [INFO] [config.py:1007:print]   amp_params ................... False\n",
            "[2025-04-25 15:24:20,914] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2025-04-25 15:24:20,915] [INFO] [config.py:1007:print]   bfloat16_enabled ............. False\n",
            "[2025-04-25 15:24:20,915] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  False\n",
            "[2025-04-25 15:24:20,916] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2025-04-25 15:24:20,917] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
            "[2025-04-25 15:24:20,918] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
            "[2025-04-25 15:24:20,919] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7afe2c1b6810>\n",
            "[2025-04-25 15:24:20,920] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
            "[2025-04-25 15:24:20,920] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
            "[2025-04-25 15:24:20,921] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2025-04-25 15:24:20,922] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
            "[2025-04-25 15:24:20,923] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
            "[2025-04-25 15:24:20,923] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2025-04-25 15:24:20,924] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
            "[2025-04-25 15:24:20,925] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
            "[2025-04-25 15:24:20,926] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
            "[2025-04-25 15:24:20,926] [INFO] [config.py:1007:print]   dump_state ................... False\n",
            "[2025-04-25 15:24:20,927] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n",
            "[2025-04-25 15:24:20,927] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
            "[2025-04-25 15:24:20,928] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2025-04-25 15:24:20,928] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2025-04-25 15:24:20,929] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
            "[2025-04-25 15:24:20,930] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
            "[2025-04-25 15:24:20,930] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
            "[2025-04-25 15:24:20,935] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading extension module fused_adam...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-04-25 15:24:20,936] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
            "[2025-04-25 15:24:20,937] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
            "[2025-04-25 15:24:20,937] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2025-04-25 15:24:20,938] [INFO] [config.py:1007:print]   fp16_auto_cast ............... False\n",
            "[2025-04-25 15:24:20,938] [INFO] [config.py:1007:print]   fp16_enabled ................. True\n",
            "[2025-04-25 15:24:20,939] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
            "[2025-04-25 15:24:20,939] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
            "[2025-04-25 15:24:20,940] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
            "[2025-04-25 15:24:20,940] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1\n",
            "[2025-04-25 15:24:20,941] [INFO] [config.py:1007:print]   gradient_clipping ............ 0.0\n",
            "[2025-04-25 15:24:20,941] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
            "[2025-04-25 15:24:20,942] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
            "[2025-04-25 15:24:20,942] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2025-04-25 15:24:20,943] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 65536\n",
            "[2025-04-25 15:24:20,943] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
            "[2025-04-25 15:24:20,944] [INFO] [config.py:1007:print]   loss_scale ................... 0\n",
            "[2025-04-25 15:24:20,944] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
            "[2025-04-25 15:24:20,945] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
            "[2025-04-25 15:24:20,945] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
            "[2025-04-25 15:24:20,946] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
            "[2025-04-25 15:24:20,947] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2025-04-25 15:24:20,947] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
            "[2025-04-25 15:24:20,948] [INFO] [config.py:1007:print]   optimizer_name ............... adamw\n",
            "[2025-04-25 15:24:20,948] [INFO] [config.py:1007:print]   optimizer_params ............. {'weight_decay': 0.0001}\n",
            "[2025-04-25 15:24:20,949] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2025-04-25 15:24:20,949] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
            "[2025-04-25 15:24:20,950] [INFO] [config.py:1007:print]   pld_params ................... False\n",
            "[2025-04-25 15:24:20,950] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
            "[2025-04-25 15:24:20,951] [INFO] [config.py:1007:print]   scheduler_name ............... None\n",
            "[2025-04-25 15:24:20,951] [INFO] [config.py:1007:print]   scheduler_params ............. None\n",
            "[2025-04-25 15:24:20,952] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2025-04-25 15:24:20,952] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
            "[2025-04-25 15:24:20,953] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
            "[2025-04-25 15:24:20,953] [INFO] [config.py:1007:print]   steps_per_print .............. None\n",
            "[2025-04-25 15:24:20,954] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
            "[2025-04-25 15:24:20,954] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
            "[2025-04-25 15:24:20,955] [INFO] [config.py:1007:print]   train_batch_size ............. 32\n",
            "[2025-04-25 15:24:20,955] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  32\n",
            "[2025-04-25 15:24:20,956] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
            "[2025-04-25 15:24:20,956] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
            "[2025-04-25 15:24:20,957] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
            "[2025-04-25 15:24:20,957] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
            "[2025-04-25 15:24:20,958] [INFO] [config.py:1007:print]   world_size ................... 1\n",
            "[2025-04-25 15:24:20,958] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  False\n",
            "[2025-04-25 15:24:20,959] [INFO] [config.py:1007:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
            "[2025-04-25 15:24:20,959] [INFO] [config.py:1007:print]   zero_enabled ................. False\n",
            "[2025-04-25 15:24:20,960] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2025-04-25 15:24:20,960] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 0\n",
            "[2025-04-25 15:24:20,961] [INFO] [config.py:993:print_user_config]   json = {\n",
            "    \"train_batch_size\": 32, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"weight_decay\": 0.0001\n",
            "        }\n",
            "    }, \n",
            "    \"timers\": {\n",
            "        \"enabled\": false\n",
            "    }\n",
            "}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-25 15:24:23,288] [INFO] [fused_optimizer.py:392:_update_scale] \n",
            "Grad overflow on iteration 0\n",
            "[2025-04-25 15:24:23,291] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0\n",
            "[2025-04-25 15:24:23,299] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0\n",
            "[2025-04-25 15:24:23,512] [INFO] [fused_optimizer.py:392:_update_scale] \n",
            "Grad overflow on iteration 1\n",
            "[2025-04-25 15:24:23,518] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0\n",
            "[2025-04-25 15:24:23,521] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            "Epoch 1/8 — loss: 0.3417, val_acc: 82.6722%\n",
            "[2025-04-25 15:33:53,222] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 1000 iterations\n",
            "[2025-04-25 15:33:53,240] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0\n",
            "[2025-04-25 15:33:55,334] [INFO] [fused_optimizer.py:392:_update_scale] \n",
            "Grad overflow on iteration 1004\n",
            "[2025-04-25 15:33:55,339] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0\n",
            "[2025-04-25 15:33:55,340] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            "Epoch 2/8 — loss: 0.3330, val_acc: 84.9220%\n",
            "[2025-04-25 15:39:50,623] [INFO] [fused_optimizer.py:392:_update_scale] \n",
            "Grad overflow on iteration 1564\n",
            "[2025-04-25 15:39:50,628] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
            "[2025-04-25 15:39:50,631] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n",
            "Epoch 3/8 — loss: 0.3036, val_acc: 83.7576%\n",
            "Epoch 4/8 — loss: 0.2694, val_acc: 84.9813%\n",
            "[2025-04-25 15:51:01,980] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 1000 iterations\n",
            "[2025-04-25 15:51:01,993] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0\n",
            "[2025-04-25 15:54:00,060] [INFO] [fused_optimizer.py:392:_update_scale] \n",
            "Grad overflow on iteration 2931\n",
            "[2025-04-25 15:54:00,067] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
            "[2025-04-25 15:54:00,068] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n",
            "Epoch 5/8 — loss: 0.2589, val_acc: 85.1589%\n",
            "Epoch 6/8 — loss: 0.2465, val_acc: 84.8036%\n",
            "[2025-04-25 16:04:58,634] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 1000 iterations\n",
            "[2025-04-25 16:04:58,647] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0\n",
            "Epoch 7/8 — loss: 0.2319, val_acc: 85.4549%\n",
            "[2025-04-25 16:14:35,169] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 1000 iterations\n",
            "[2025-04-25 16:14:35,176] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0\n",
            "[2025-04-25 16:14:38,638] [INFO] [fused_optimizer.py:392:_update_scale] \n",
            "Grad overflow on iteration 4939\n",
            "[2025-04-25 16:14:38,639] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0\n",
            "[2025-04-25 16:14:38,645] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            "Epoch 8/8 — loss: 0.2268, val_acc: 84.2510%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:72: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert H % patch_H == 0, f\"Input image height {H} is not a multiple of patch height {patch_H}\"\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:73: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert W % patch_W == 0, f\"Input image width {W} is not a multiple of patch width: {patch_W}\"\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:183: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if npatch == N and w == h:\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:191: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  M = int(math.sqrt(N))  # Recover the number of patches in each dimension\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:192: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert N == M * M\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:197: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sx = float(w0 + self.interpolate_offset) / M\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:198: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  sy = float(h0 + self.interpolate_offset) / M\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:209: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (w0, h0) == patch_pos_embed.shape[-2:]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/jit/_trace.py:1304: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Tensor-likes are not close!\n",
            "\n",
            "Mismatched elements: 6 / 8 (75.0%)\n",
            "Greatest absolute difference: 0.0078125 at index (0, 3) (up to 1e-05 allowed)\n",
            "Greatest relative difference: 0.0019120458891013384 at index (0, 2) (up to 1e-05 allowed)\n",
            "  _check_trace(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Partial Fine-Tuning Results ===\n",
            "Accuracy:      84.2510%\n",
            "Latency:       11.96 ms\n",
            "Model Size:    173.62 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkiohaI0B-xG",
        "outputId": "a3b859cb-b9b4-4eb8-a544-48bb1e4533ee"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Results Comparison ===\n",
            "Scheme                                     Top1 Acc    Infer(ms)   Size(MB)\n",
            "--------------------------------------------------------------------\n",
            "1. Linear Probing                            77.56%        20.40     346.79\n",
            "2. Linear + Partial FT                       83.38%        20.35     346.79\n",
            "3. + GPU FLOAT16 Quantization                83.34%         9.84     173.63\n",
            "4. + GPU FLOAT16 Quantization FT             84.25%        11.96     173.62\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Results Comparison ===\")\n",
        "print(f\"{'Scheme':<40} {'Top1 Acc':>10}   {'Infer(ms)':>10}   {'Size(MB)':>8}\")\n",
        "print(\"-\"*68)\n",
        "print(f\"{'1. Linear Probing':<40} {acc_lp*100:>9.2f}%   {time_lp:>10.2f}   {size_lp:>8.2f}\")\n",
        "print(f\"{'2. Linear + Partial FT':<40} {acc_ft*100:>9.2f}%   {time_ft:>10.2f}   {size_ft:>8.2f}\")\n",
        "# print(f\"{'3. + GPU INT8 Quantization':<40} {ds_acc_8qt*100:>9.2f}%   {ds_time_8qt:>10.2f}   {ds_size_8qt:>8.2f}\")\n",
        "print(f\"{'3. + GPU FLOAT16 Quantization':<40} {ds_acc_16qt*100:>9.2f}%   {ds_time_16qt:>10.2f}   {ds_size_16qt:>8.2f}\")\n",
        "print(f\"{'4. + GPU FLOAT16 Quantization FT':<40} {ds_acc_16qt_ft*100:>9.2f}%   {ds_time_16qt_ft:>10.2f}   {ds_size_16qt_ft:>8.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lp.eval()\n",
        "ds_model_8qt = deepspeed.init_inference(\n",
        "    model_lp,\n",
        "    mp_size=1,                       # 单 GPU\n",
        "    dtype=torch.int8,                # 量化到 INT8\n",
        "    replace_method='auto',           # 自动定位哪些层可量化\n",
        "    replace_with_kernel_inject=True  # 注入高性能量化内核\n",
        ")\n",
        "\n",
        "# 3) 移到 GPU\n",
        "ds_model_8qt.cuda()\n",
        "\n",
        "# 4) 直接用 ds_model_8qt 做推理\n",
        "example = torch.randn(1, 3, 224, 224, device=device)\n",
        "ds_mod_8qt_trt = torch.jit.trace(ds_model_8qt, example)\n",
        "\n",
        "torch.jit.save(ds_mod_8qt_trt, \"model_ds_int8.ts\")\n",
        "\n",
        "ds_acc_8qt = evaluate(ds_model_8qt, val_loader, device)\n",
        "ds_time_8qt = measure_inference_time(ds_model_8qt, device)\n",
        "ds_size_8qt = os.path.getsize(\"model_ds_int8.ts\")/1e6\n",
        "\n",
        "print(\"\\n=== Deepspeed int8 Quantization Results ===\")\n",
        "print(f\"Accuracy: {ds_acc_8qt:.4%}, Inference Time: {ds_time_8qt:.2f}ms, Model Size: {ds_size_8qt:.2f}MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "CE4aiBOfNfoq",
        "outputId": "6f6323eb-82d0-4d7e-c4d8-713554161ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-25 16:18:26,555] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n",
            "[2025-04-25 16:18:26,584] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2025-04-25 16:18:26,585] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Data type torch.int8 is not supported by cuda accelerator",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0e91694d5481>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_lp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m ds_model_8qt = deepspeed.init_inference(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_lp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmp_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;31m# 单 GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m# 量化到 INT8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepspeed/__init__.py\u001b[0m in \u001b[0;36minit_inference\u001b[0;34m(model, config, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mds_inference_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepSpeedInferenceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInferenceEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_inference_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepspeed/inference/engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, config)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     72\u001b[0m                 f\"Data type {config.dtype} is not supported by {get_accelerator().device_name()} accelerator\")\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data type torch.int8 is not supported by cuda accelerator"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KblLORQjB-xF"
      },
      "outputs": [],
      "source": [
        "# ========== TensorRT INT8 ==========\n",
        "class CalibrationDataLoader(object):\n",
        "    def __init__(self, dataloader, max_samples=100):\n",
        "        self.dataloader = dataloader\n",
        "        self.max_samples = max_samples\n",
        "        self.current = 0\n",
        "        self.images = []\n",
        "\n",
        "\n",
        "        for images, _ in dataloader:\n",
        "            self.images.extend(images)\n",
        "            if len(self.images) >= max_samples:\n",
        "                break\n",
        "        self.images = self.images[:max_samples]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.current = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current < len(self.images):\n",
        "            data = self.images[self.current].to(device)\n",
        "            self.current += 1\n",
        "            return [data]\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "calib_dataloader = CalibrationDataLoader(train_loader, max_samples=100)\n",
        "\n",
        "ts_mod_trt = torch_tensorrt.compile(ts_mod,\n",
        "    inputs = [torch_tensorrt.Input(\n",
        "        min_shape=[1, 3, 224, 224],\n",
        "        opt_shape=[BATCH_SIZE, 3, 224, 224],\n",
        "        max_shape=[BATCH_SIZE*2, 3, 224, 224],\n",
        "        dtype=torch.float32\n",
        "    )],\n",
        "    enabled_precisions={torch.int8}, # 启用INT8精度\n",
        "    calibrator=torch_tensorrt.ptq.DataLoaderCalibrator(\n",
        "        calib_dataloader,\n",
        "        cache_file=\"./calibration.cache\",\n",
        "        use_cache=False,\n",
        "        algo_type=torch_tensorrt.ptq.CalibrationAlgo.ENTROPY_CALIBRATION_2,\n",
        "    ),\n",
        "    workspace_size=1 << 22\n",
        ")\n",
        "\n",
        "torch.jit.save(ts_mod_trt, \"model_trt_int8.ts\")\n",
        "\n",
        "acc_qt = evaluate(ts_mod_trt, val_loader, device)\n",
        "time_qt = measure_inference_time(ts_mod_trt, device)\n",
        "size_qt = os.path.getsize(\"model_trt_int8.ts\")/1e6\n",
        "\n",
        "print(\"\\n=== Quantization Results ===\")\n",
        "print(f\"Accuracy: {acc_qt:.4%}, Inference Time: {time_qt:.2f}ms, Model Size: {size_qt:.2f}MB\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}