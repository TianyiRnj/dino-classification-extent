{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script loads the HAM10000 skin lesion dataset and performs fine-tuning on DinoV2 ViT-B/14 model.\n",
    "It compares the performance between the pre-trained model without fine-tuning and with fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/anaconda3/envs/785project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "# from torchvision.models import dinov2_vitb14, DINOv2_ViTB14_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "DATA_DIR = \"/home/zack/11785/project/data/HAM10000_dataset\"\n",
    "METADATA_FILE = os.path.join(DATA_DIR, \"HAM10000_metadata.csv\")\n",
    "\n",
    "# Create HAM10000 Dataset\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: Pandas dataframe containing image_id and dx (diagnosis)\n",
    "            img_dir: Directory with all the images\n",
    "            transform: Optional transform to apply to the images\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Create a mapping from diagnosis to class index\n",
    "        self.diagnosis_mapping = {\n",
    "            'akiec': 0,  # Actinic Keratosis\n",
    "            'bcc': 1,    # Basal Cell Carcinoma\n",
    "            'bkl': 2,    # Benign Keratosis\n",
    "            'df': 3,     # Dermatofibroma\n",
    "            'mel': 4,    # Melanoma\n",
    "            'nv': 5,     # Melanocytic Nevus\n",
    "            'vasc': 6    # Vascular Lesion\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.dataframe.iloc[idx]['image_id']\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.diagnosis_mapping[self.dataframe.iloc[idx]['dx']]\n",
    "        return image, label\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def load_data():\n",
    "    # Read metadata\n",
    "    metadata = pd.read_csv(METADATA_FILE)\n",
    "    print(f\"Dataset size: {len(metadata)}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    print(\"Class distribution:\")\n",
    "    print(metadata['dx'].value_counts())\n",
    "    \n",
    "    # Train/test split\n",
    "    train_df, test_df = train_test_split(metadata, test_size=0.2, stratify=metadata['dx'], random_state=42)\n",
    "    \n",
    "    # Define transformations\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = HAM10000Dataset(train_df, DATA_DIR, train_transform)\n",
    "    test_dataset = HAM10000Dataset(test_df, DATA_DIR, test_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier on top of DinoV2 features\n",
    "class DinoClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=7, fine_tune=True):\n",
    "        super(DinoClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        # Freeze backbone if not fine-tuning\n",
    "        if not fine_tune:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            # Only fine-tune the last 2 blocks of the backbone\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                if 'blocks.10' in name or 'blocks.11' in name or 'norm' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Get embedding dimension from the backbone\n",
    "        if hasattr(self.backbone, 'embed_dim'):\n",
    "            embedding_dim = self.backbone.embed_dim\n",
    "        else:\n",
    "            embedding_dim = self.backbone.num_features\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using the backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Pass features through the classifier\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_accuracy = 0.0\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    scaler = torch.cuda.amp.GradScaler()  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()        \n",
    "            scaler.step(optimizer)                \n",
    "            scaler.update()                       \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                # 使用混合精度进行推理\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return train_losses, test_accuracies, best_accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10015\n",
      "Class distribution:\n",
      "dx\n",
      "nv       6705\n",
      "mel      1113\n",
      "bkl      1099\n",
      "bcc       514\n",
      "akiec     327\n",
      "vasc      142\n",
      "df        115\n",
      "Name: count, dtype: int64\n",
      "Loading DinoV2 ViT-B/14 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zack/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded DinoV2 from torch\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data()\n",
    "\n",
    "print(\"Loading DinoV2 ViT-B/14 model...\")\n",
    "try:\n",
    "    backbone =  torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "    \n",
    "    print(\"Successfully loaded DinoV2 from torch\")\n",
    "except:\n",
    "    backbone = timm.create_model('vit_base_patch14_dinov2', pretrained=True)\n",
    "    print(\"Successfully loaded DinoV2 from timm\")\n",
    "\n",
    "# Model without fine-tuning (frozen backbone)\n",
    "frozen_model = DinoClassifier(backbone, fine_tune=False).to(device)\n",
    "\n",
    "# Model with fine-tuning\n",
    "finetuned_model = DinoClassifier(backbone, fine_tune=True).to(device)\n",
    "\n",
    "# Define loss function and optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "frozen_optimizer = optim.AdamW(frozen_model.parameters(), lr=1e-3)\n",
    "finetuned_optimizer = optim.AdamW([\n",
    "    {'params': [p for n, p in finetuned_model.named_parameters() if 'backbone' in n and p.requires_grad], 'lr': 5e-5},\n",
    "    {'params': [p for n, p in finetuned_model.named_parameters() if 'backbone' not in n], 'lr': 1e-3}\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model with frozen backbone ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 251/251 [01:40<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0187, Test Accuracy: 0.7064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 251/251 [01:45<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.8224, Test Accuracy: 0.7059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 251/251 [01:42<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.7659, Test Accuracy: 0.7299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 251/251 [01:42<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.7362, Test Accuracy: 0.7364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 251/251 [01:42<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.6931, Test Accuracy: 0.7384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 251/251 [01:42<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.6654, Test Accuracy: 0.7649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 251/251 [01:42<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.6476, Test Accuracy: 0.7703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 251/251 [01:42<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.5999, Test Accuracy: 0.7843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 251/251 [01:43<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.5821, Test Accuracy: 0.7329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 251/251 [01:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.5719, Test Accuracy: 0.7908\n",
      "\n",
      "--- Final evaluation of model with frozen backbone ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [00:11<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7908\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.41      0.37      0.39        65\n",
      "         bcc       0.61      0.71      0.66       103\n",
      "         bkl       0.55      0.58      0.57       220\n",
      "          df       0.67      0.09      0.15        23\n",
      "         mel       0.49      0.52      0.51       223\n",
      "          nv       0.92      0.91      0.91      1341\n",
      "        vasc       0.88      0.79      0.83        28\n",
      "\n",
      "    accuracy                           0.79      2003\n",
      "   macro avg       0.65      0.57      0.57      2003\n",
      "weighted avg       0.79      0.79      0.79      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate frozen model\n",
    "print(\"\\n--- Training model with frozen backbone ---\")\n",
    "frozen_losses, frozen_accuracies, frozen_best = train_model(\n",
    "    frozen_model, train_loader, test_loader, criterion, frozen_optimizer, num_epochs=10\n",
    ")\n",
    "\n",
    "# Load best frozen model and evaluate\n",
    "frozen_model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"\\n--- Final evaluation of model with frozen backbone ---\")\n",
    "frozen_accuracy, frozen_report = evaluate_model(frozen_model, test_loader)\n",
    "\n",
    "# Rename the best model file to avoid overwriting\n",
    "os.rename('best_model.pth', 'frozen_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model with fine-tuned backbone ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 251/251 [01:43<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5200, Test Accuracy: 0.7823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 251/251 [01:43<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.5071, Test Accuracy: 0.8018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 251/251 [01:45<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.4895, Test Accuracy: 0.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 251/251 [01:46<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.4782, Test Accuracy: 0.8043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 251/251 [01:47<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.4770, Test Accuracy: 0.8123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 251/251 [01:44<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.4742, Test Accuracy: 0.8093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 251/251 [01:46<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.4711, Test Accuracy: 0.8173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 251/251 [01:45<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.4606, Test Accuracy: 0.8153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 251/251 [01:42<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.4586, Test Accuracy: 0.8173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 251/251 [01:42<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.4616, Test Accuracy: 0.8223\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate fine-tuned model\n",
    "print(\"\\n--- Training model with fine-tuned backbone ---\")\n",
    "finetuned_losses, finetuned_accuracies, finetuned_best = train_model(\n",
    "    finetuned_model, train_loader, test_loader, criterion, finetuned_optimizer, num_epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final evaluation of model with fine-tuned backbone ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63/63 [00:11<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8223\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.63      0.42      0.50        65\n",
      "         bcc       0.64      0.79      0.70       103\n",
      "         bkl       0.65      0.64      0.64       220\n",
      "          df       0.50      0.57      0.53        23\n",
      "         mel       0.58      0.46      0.52       223\n",
      "          nv       0.91      0.94      0.92      1341\n",
      "        vasc       0.91      0.75      0.82        28\n",
      "\n",
      "    accuracy                           0.82      2003\n",
      "   macro avg       0.69      0.65      0.66      2003\n",
      "weighted avg       0.82      0.82      0.82      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best fine-tuned model and evaluate\n",
    "finetuned_model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"\\n--- Final evaluation of model with fine-tuned backbone ---\")\n",
    "finetuned_accuracy, finetuned_report = evaluate_model(finetuned_model, test_loader)\n",
    "\n",
    "# Rename the fine-tuned model file for clarity\n",
    "os.rename('best_model.pth', 'finetuned_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison ---\n",
      "Frozen model best accuracy: 0.7908\n",
      "Fine-tuned model best accuracy: 0.8223\n",
      "Improvement: 3.15%\n"
     ]
    }
   ],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(frozen_losses) + 1), frozen_losses, label='Frozen')\n",
    "plt.plot(range(1, len(finetuned_losses) + 1), finetuned_losses, label='Fine-tuned')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(frozen_accuracies) + 1), frozen_accuracies, label='Frozen')\n",
    "plt.plot(range(1, len(finetuned_accuracies) + 1), finetuned_accuracies, label='Fine-tuned')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Frozen model best accuracy: {frozen_best:.4f}\")\n",
    "print(f\"Fine-tuned model best accuracy: {finetuned_best:.4f}\")\n",
    "print(f\"Improvement: {(finetuned_best - frozen_best) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "785project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
